import pytse_client as tse
import pandas as pd
from typing import List, Optional, Tuple, Dict
from datetime import date, datetime
from sqlalchemy.orm import Session
from sqlalchemy import func, or_, text
from sqlalchemy.exc import SQLAlchemyError
import jdatetime
import traceback
import numpy as np
import time
import logging

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ùˆ Import Ù‡Ø§ÛŒ ÙØ±Ø¶ÛŒ ---

# ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¯Ø± models.py ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
# **ØªÙˆØ¬Ù‡:** Ø¯Ø± Ù…Ø­ÛŒØ· ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø§ÛŒÙ† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ù…Ø«Ù„ db.Model) import Ø´ÙˆÙ†Ø¯.
class HistoricalData:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)
    def __repr__(self):
        return f'<HistoricalData {self.symbol_name}>'

class FundamentalData:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)
    def __repr__(self):
        return f'<FundamentalData {self.symbol_id}>'

class ComprehensiveSymbolData:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)
        self.last_historical_update_date = None
    def __repr__(self):
        return f'<ComprehensiveSymbolData {self.symbol_name}>'

# ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙˆØ§Ø¨Ø¹ Ùˆ Logger Ø¯Ø± services.utils ÛŒØ§ Ø¬Ø§ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # ØªÙ†Ø¸ÛŒÙ… Ø³Ø·Ø­ Ù„Ø§Ú¯ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§

def safe_sleep(delay: float):
    """Ù…Ú©Ø« Ø§ÛŒÙ…Ù† Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ø´Ø¨Ú©Ù‡"""
    time.sleep(delay)

DEFAULT_PER_SYMBOL_DELAY = 1.5 # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ù‡Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ticker
SYMBOL_BATCH_SIZE = 100

# **ØªÙˆØ§Ø¨Ø¹ ØªØ­Ù„ÛŒÙ„ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Placeholder** (ÙØ±Ø¶ Ø¨Ø± import Ø§Ø² services.data_fetch_and_process.py)
# Ø§ÛŒÙ† ØªÙˆØ§Ø¨Ø¹ Ø¨Ø§ÛŒØ¯ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯.
def run_technical_analysis(db_session, limit: int = None, symbols_list: list = None): 
    logger.info("ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ run_technical_analysis...")
    return len(symbols_list) if symbols_list else 0, "ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡"

def run_candlestick_detection(db_session, limit: int = None, symbols_list: list = None): 
    logger.info("ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ run_candlestick_detection...")
    return len(symbols_list) if symbols_list else 0

# --------------------------------------------------------------------------------
# 2. ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
# --------------------------------------------------------------------------------

def delete_all_historical_data(db_session: Session) -> int:
    """
    Ø­Ø°Ù ØªÙ…Ø§Ù… Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ HistoricalData.
    Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² TRUNCATE Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯.
    """
    try:
        logger.warning("ğŸ—‘ï¸ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø°Ù ØªÙ…Ø§Ù… Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ HistoricalData Ø¨Ø§ TRUNCATE...")
        # ØªÙˆØ¬Ù‡: Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± Ø¨Ø±Ø§ÛŒ PostgreSQL Ùˆ MySQL (Ø¨Ø§ NO CHECK) Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        db_session.execute(text("TRUNCATE TABLE stock_data RESTART IDENTITY CASCADE;"))
        db_session.commit()
        logger.info(f"âœ… Ø¬Ø¯ÙˆÙ„ HistoricalData (stock_data) Ø®Ø§Ù„ÛŒ Ø´Ø¯.")
        return 0 
    except SQLAlchemyError as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ø­Ø°Ù HistoricalData: {e}", exc_info=True)
        db_session.rollback()
        raise

def update_fundamental_data(db_session: Session, ticker: tse.Ticker, symbol_id: str) -> bool:
    """
    Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ØŒ Ù‡Ù†Ø¯Ù„ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ùˆ Ø°Ø®ÛŒØ±Ù‡/Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø± FundamentalData.
    """
    try:
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Session Ù‚Ø¨Ù„ Ø§Ø² Merge Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§Ù‡Ø§ÛŒ Integrity
        db_session.expire_all()
        
        data = {
            'eps': getattr(ticker, 'eps', None),
            'pe': getattr(ticker, 'p_e_ratio', None),
            'group_pe_ratio': getattr(ticker, 'group_p_e_ratio', None),
            'psr': getattr(ticker, 'psr', None),
            'p_s_ratio': getattr(ticker, 'p_s_ratio', None),
            'base_volume': getattr(ticker, 'base_volume', None),
            'float_shares': getattr(ticker, 'float_shares', None),
            'market_cap': getattr(ticker, 'market_cap', None),
        }
        
        # Ù‡Ù†Ø¯Ù„ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø± ('-', '--') Ùˆ ØªØ¨Ø¯ÛŒÙ„ Market Cap
        for key, value in data.items():
            if isinstance(value, str):
                cleaned_value = value.strip()
                if cleaned_value in ['-', '--', 'nan', ''] or pd.isna(value):
                    data[key] = None
                elif key == 'market_cap':
                    try:
                        data[key] = int(cleaned_value.replace(',', ''))
                    except:
                        data[key] = None
            elif pd.isna(value) or value in (np.nan, np.inf, -np.inf):
                data[key] = None
            
        fundamental_record = FundamentalData(
            symbol_id=symbol_id,
            eps=data['eps'],
            pe=data['pe'],
            group_pe_ratio=data['group_pe_ratio'],
            psr=data['psr'],
            p_s_ratio=data['p_s_ratio'],
            market_cap=data['market_cap'],
            base_volume=data['base_volume'],
            float_shares=data['float_shares']
        )
        
        db_session.merge(fundamental_record)
        return True
    
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ FundamentalData Ø¨Ø±Ø§ÛŒ {symbol_id}: {e}", exc_info=True)
        return False

def _commit_historical_batch(db_session: Session, records_to_commit: List[HistoricalData], symbol_ids_to_update: set) -> Tuple[int, bool]:
    """ÛŒÚ© Ø¨Ú† Ø§Ø² Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ HistoricalData Ø±Ø§ Ø¯Ø±Ø¬ Ú©Ø±Ø¯Ù‡ Ùˆ ComprehensiveSymbolData Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ² Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    if not records_to_commit:
        return 0, True
        
    try:
        logger.info(f"â³ Ø´Ø±ÙˆØ¹ Ø¯Ø±Ø¬ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ú† {len(symbol_ids_to_update)} Ù†Ù…Ø§Ø¯ ({len(records_to_commit)} Ø±Ú©ÙˆØ±Ø¯)...")
        # Bulk insert
        db_session.bulk_save_objects(records_to_commit)
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² datetime.now() Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ† DateTime
        current_timestamp = datetime.now()
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ØªØ§Ø±ÛŒØ® Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø± ComprehensiveSymbolData Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ú†
        db_session.query(ComprehensiveSymbolData).filter(
            ComprehensiveSymbolData.symbol_id.in_(list(symbol_ids_to_update))
        ).update(
            {ComprehensiveSymbolData.last_historical_update_date: current_timestamp}, 
            synchronize_session='fetch'
        )
        
        db_session.commit()
        logger.info(f"âœ… Ø¨Ú† {len(symbol_ids_to_update)} Ù†Ù…Ø§Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ú©Ø§Ù…ÛŒØª Ø´Ø¯.")
        return len(records_to_commit), True
        
    except SQLAlchemyError as e:
        db_session.rollback()
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¯Ø± Ø¯Ø±Ø¬ Ø¨Ú† {len(symbol_ids_to_update)} Ù†Ù…Ø§Ø¯. Rollback Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.: {e}", exc_info=True)
        return 0, False
    except Exception as e:
        db_session.rollback()
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ø¯Ø±Ø¬ Ø¨Ú†. Rollback Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.: {e}", exc_info=True)
        return 0, False


# --------------------------------------------------------------------------------
# 3. ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø§ Ù…Ú©Ø§Ù†ÛŒØ²Ù… Ø¨Ú†â€ŒØ¨Ù†Ø¯ÛŒ
# --------------------------------------------------------------------------------
def fetch_full_historical_pytse(
    db_session: Session,
    symbols_to_update: Optional[List[str]] = None
) -> Tuple[int, str]:
    """
    Ø±ÙØ±Ø´ Ú©Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ø¨Ø§ Ù…Ú©Ø§Ù†ÛŒØ²Ù… Ø¨Ú†â€ŒØ¨Ù†Ø¯ÛŒ Û±Û°Û° ØªØ§ÛŒÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§.
    """
    logger.info("ğŸ§  Ø´Ø±ÙˆØ¹ Ø±ÙØ±Ø´ Ú©Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ (Full Historical Refresh) Ø¨Ø§ Ø¨Ú†â€ŒØ¨Ù†Ø¯ÛŒ...")
    
    # 1. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§
    try:
        query = db_session.query(ComprehensiveSymbolData).filter(
            ComprehensiveSymbolData.symbol_name != None 
        )
        if symbols_to_update:
            query = query.filter(ComprehensiveSymbolData.symbol_name.in_(symbols_to_update))
            
        all_symbols = query.all()
        
        if not all_symbols:
            return 0, "âŒ Ù‡ÛŒÚ† Ù†Ù…Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø± ComprehensiveSymbolData ÛŒØ§ÙØª Ù†Ø´Ø¯."
            
        # 2. Ø­Ø°Ù ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ HistoricalData
        delete_all_historical_data(db_session)

    except Exception as e:
        return 0, f"âŒ Ø®Ø·Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ÛŒØ§ Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…Ø§Ø¯Ù‡Ø§: {e}"

    # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ú†
    current_batch_records = []
    current_batch_symbol_ids = set()
    total_records_inserted = 0
    total_symbols = len(all_symbols)
    updated_symbol_ids = set() 

    # 3. Ø­Ù„Ù‚Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù…Ø§Ø¯ Ø¨Ù‡ Ù†Ù…Ø§Ø¯
    for index, sym in enumerate(all_symbols):
        symbol_name = sym.symbol_name
        symbol_id = sym.symbol_id

        if not symbol_name:
             logger.warning(f"âš ï¸ Ù†Ù…Ø§Ø¯ Ø¨Ø§ ID {symbol_id} Ù†Ø§Ù… Ù†Ø¯Ø§Ø±Ø¯. Ø±Ø¯ Ø´Ø¯.")
             continue

        logger.info(f"ğŸ“Š Ù¾Ø±Ø¯Ø§Ø²Ø´ ({index+1}/{total_symbols}) Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ {symbol_name} (ID: {symbol_id})")
        
        try:
            # --- ÙØ§Ø² ÙÚ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø´Ø¨Ú©Ù‡) ---
            
            # Ø§Ù„Ù) Ø¯Ø±ÛŒØ§ÙØª Ticker Ø¨Ø±Ø§ÛŒ Final Price Ùˆ Fundamental Data
            ticker_adj = tse.Ticker(symbol_name, adjust=True)
            
            # Sleep Ø¨ÛŒÙ† Ø¯Ùˆ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ticker Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø´Ø¨Ú©Ù‡
            safe_sleep(0.5) 
            
            # Ø¨) Ø¯Ø±ÛŒØ§ÙØª Ticker Ø¨Ø±Ø§ÛŒ Last Price
            ticker_unadj = tse.Ticker(symbol_name, adjust=False)
            
            df_adj = ticker_adj.history.copy()
            df_unadj = ticker_unadj.history.copy()
            
            # Ù‡Ù†Ø¯Ù„ Ú©Ø±Ø¯Ù† None Ø¨Ø±Ø§ÛŒ client_types
            df_client_types = ticker_adj.client_types.copy() if ticker_adj.client_types is not None else pd.DataFrame()
            
            if df_adj is None or df_adj.empty or df_unadj is None or df_unadj.empty:
                logger.info(f"â„¹ï¸ Ø¯ÛŒØªØ§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_name} ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø±Ø¯ Ø´Ø¯.")
                safe_sleep(DEFAULT_PER_SYMBOL_DELAY)
                continue
            
            # 4. Ø¢Ù¾Ø¯ÛŒØª FundamentalData
            update_fundamental_data(db_session, ticker_adj, symbol_id)

            # --- ÙØ§Ø² Ø§Ø¯ØºØ§Ù…ØŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ ---
            
            # ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ Ø¨Ù‡ datetime Ùˆ Ø§Ø¯ØºØ§Ù…
            df_adj['date'] = pd.to_datetime(df_adj['date'])
            df_unadj['date'] = pd.to_datetime(df_unadj['date'])
            if 'date' in df_client_types.columns:
                df_client_types['date'] = pd.to_datetime(df_client_types['date'])
            
            # Ú†Ú© Ú©Ø±Ø¯Ù† 'adj_close' Ù‚Ø¨Ù„ Ø§Ø² ØªØºÛŒÛŒØ± Ù†Ø§Ù…
            if 'adj_close' in df_adj.columns: 
                df_adj.rename(columns={'adj_close': 'final_price_value'}, inplace=True)
            else:
                df_adj.rename(columns={'close': 'final_price_value'}, inplace=True)

            df_unadj.rename(columns={'close': 'last_price_value'}, inplace=True)
            
            df_merged = pd.merge(
                df_adj, df_unadj[['date', 'last_price_value']], 
                on='date', how='inner'
            )
            
            df_final = pd.merge(df_merged, df_client_types, on='date', how='left')

            # 5. Ø§ØµÙ„Ø§Ø­ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ (PLC, PLP, PCC, PCP Ùˆ Ù†Ú¯Ø§Ø´Øª)
            df_final.sort_values(by='date', inplace=True)
            
            df_final['final'] = pd.to_numeric(df_final['final_price_value'], errors='coerce')
            df_final['close'] = pd.to_numeric(df_final['last_price_value'], errors='coerce')
            
            # yesterday_price Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÛŒÙ…Øª Ù¾Ø§ÛŒØ§Ù†ÛŒ ØªØ¹Ø¯ÛŒÙ„â€ŒØ´Ø¯Ù‡ Ø§Ø³Øª
            df_final['yesterday_price'] = df_final['final'].shift(1) 
            
            # Price Changes (Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ± Ø¨Ø§ replace(0, np.nan))
            df_final['pcc'] = df_final['final'] - df_final['yesterday_price']
            df_final['pcp'] = (df_final['pcc'] / df_final['yesterday_price'].replace(0, np.nan)) * 100
            
            df_final['plc'] = df_final['close'] - df_final['yesterday_price']
            df_final['plp'] = (df_final['plc'] / df_final['yesterday_price'].replace(0, np.nan)) * 100
            
            # Ø³Ø§ÛŒØ± Ù†Ú¯Ø§Ø´Øªâ€ŒÙ‡Ø§
            df_final['mv'] = df_final['value']
            df_final['num_trades'] = df_final['count'] 
            
            # Ù†Ú¯Ø§Ø´Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÛŒÙ‚ÛŒ/Ø­Ù‚ÙˆÙ‚ÛŒ
            df_final.rename(columns={
                "individual_buy_count": "buy_count_i", "corporate_buy_count": "buy_count_n",
                "individual_sell_count": "sell_count_i", "corporate_sell_count": "sell_count_n",
                "individual_buy_vol": "buy_i_volume", "corporate_buy_vol": "buy_n_volume",
                "individual_sell_vol": "sell_i_volume", "corporate_sell_vol": "sell_n_volume"
            }, inplace=True) 

            # ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® Ù…ÛŒÙ„Ø§Ø¯ÛŒ Ø¨Ù‡ Ø´Ù…Ø³ÛŒ
            df_final['jdate'] = df_final['date'].apply(
                lambda x: jdatetime.date.fromgregorian(date=x.date()).strftime("%Y-%m-%d")
            )
            
            # 6. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø¬ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ (Bulk Insert)
            db_columns_to_keep = [
                'date', 'jdate', 'open', 'high', 'low', 'close', 'final', 
                'volume', 'value', 'num_trades', 'yesterday_price', 
                'plc', 'plp', 'pcc', 'pcp', 'mv',
                'buy_count_i', 'buy_count_n', 'sell_count_i', 'sell_count_n', 
                'buy_i_volume', 'buy_n_volume', 'sell_i_volume', 'sell_n_volume',
            ]
            
            final_data = df_final[[col for col in db_columns_to_keep if col in df_final.columns]].copy()
            final_data.replace([np.inf, -np.inf, np.nan], None, inplace=True)
            records_dict = final_data.to_dict('records')
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ù‡ Ø¢Ø¨Ø¬Ú©Øªâ€ŒÙ‡Ø§ÛŒ HistoricalData
            historical_records = [
                HistoricalData(
                    symbol_id=symbol_id, symbol_name=sym.symbol_name,
                    date=rec['date'].date() if rec.get('date') is not None else None, 
                    jdate=rec.get('jdate'), 
                    open=rec.get('open'), high=rec.get('high'), low=rec.get('low'),
                    close=rec.get('close'), final=rec.get('final'), 
                    yesterday_price=rec.get('yesterday_price'), 
                    volume=rec.get('volume'), value=rec.get('value'),
                    num_trades=rec.get('num_trades'), mv=rec.get('mv'),
                    plc=rec.get('plc'), plp=rec.get('plp'), pcc=rec.get('pcc'), pcp=rec.get('pcp'),
                    buy_count_i=rec.get('buy_count_i'), buy_count_n=rec.get('buy_count_n'), 
                    sell_count_i=rec.get('sell_count_i'), sell_count_n=rec.get('sell_count_n'), 
                    buy_i_volume=rec.get('buy_i_volume'), buy_n_volume=rec.get('buy_n_volume'), 
                    sell_i_volume=rec.get('sell_i_volume'), sell_n_volume=rec.get('sell_n_volume'),
                    # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Order Book Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± None
                    zd1=None, qd1=None, pd1=None, zo1=None, qo1=None, po1=None, 
                    zd2=None, qd2=None, pd2=None, zo2=None, qo2=None, po2=None, 
                    zd3=None, qd3=None, pd3=None, zo3=None, qo3=None, po3=None, 
                    zd4=None, qd4=None, pd4=None, zo4=None, qo4=None, po4=None, 
                    zd5=None, qd5=None, pd5=None, zo5=None, qo5=None, po5=None
                ) for rec in records_dict
            ]
            
            if historical_records:
                current_batch_records.extend(historical_records)
                current_batch_symbol_ids.add(symbol_id)
                updated_symbol_ids.add(symbol_id)
                logger.info(f"ğŸ’¾ {len(historical_records)} Ø±Ú©ÙˆØ±Ø¯ Ø¨Ø±Ø§ÛŒ {symbol_name} Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯. (Ø¨Ú†: {len(current_batch_symbol_ids)}/{SYMBOL_BATCH_SIZE})")

                # 7. Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ú†â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ Ú©Ø§Ù…ÛŒØª
                if len(current_batch_symbol_ids) >= SYMBOL_BATCH_SIZE:
                    inserted_count, success = _commit_historical_batch(
                        db_session, current_batch_records, current_batch_symbol_ids
                    )
                    total_records_inserted += inserted_count
                    
                    # Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¨Ú†
                    current_batch_records = []
                    current_batch_symbol_ids = set()
                    
            # Ø§Ø³ØªØ±Ø§Ø­Øª Ú©ÙˆØªØ§Ù‡
            safe_sleep(DEFAULT_PER_SYMBOL_DELAY)
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_name}. Ù†Ù…Ø§Ø¯ Ø±Ø¯ Ø´Ø¯. Ø®Ø·Ø§: {e}", exc_info=True)
            safe_sleep(DEFAULT_PER_SYMBOL_DELAY * 2) 
            continue 

    # 8. Ø¯Ø±Ø¬ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ (Last Batch)
    if current_batch_records:
        inserted_count, success = _commit_historical_batch(
            db_session, current_batch_records, current_batch_symbol_ids
        )
        total_records_inserted += inserted_count
        
    logger.info(f"âœ… Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ú†â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§: {total_records_inserted}")

    # 9. ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ùˆ Ú©Ù†Ø¯Ù„ Ø§Ø³ØªÛŒÚ©
    symbol_ids_list = list(updated_symbol_ids)
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù¾Ø§Ú© Ø¨ÙˆØ¯Ù† Session Ù‚Ø¨Ù„ Ø§Ø² ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªÙˆØ§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ
    db_session.commit()
    
    # Ø§Ù„Ù) Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
    tech_count, tech_msg = run_technical_analysis(db_session, symbols_list=symbol_ids_list)
    logger.info(f"Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {tech_msg}")
    
    # Ø¨) Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ
    candle_count = run_candlestick_detection(db_session, symbols_list=symbol_ids_list)
    logger.info(f"Ú¯Ø²Ø§Ø±Ø´ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {candle_count} Ù†Ù…Ø§Ø¯ Ø¨Ø§ Ø§Ù„Ú¯Ùˆ.")
        
    message = f"âœ… Ø±ÙØ±Ø´ Ú©Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ (Full Historical Refresh) Ú©Ø§Ù…Ù„ Ø´Ø¯. {total_records_inserted} Ø±Ú©ÙˆØ±Ø¯ ØªØ§Ø±ÛŒØ®ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¯Ø±Ø¬ Ø´Ø¯. {tech_count} Ù†Ù…Ø§Ø¯ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø´Ø¯Ù†Ø¯. {candle_count} Ù†Ù…Ø§Ø¯ Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù…Ø¹ÛŒ Ø¯Ø§Ø´ØªÙ†Ø¯."
    logger.info(message)
    
    return total_records_inserted, message