# -*- coding: utf-8 -*-
# services/data_fetch_and_process.py
# Ù†Ø³Ø®Ù‡ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ Ø¨Ø§ ØªÙˆØ§Ø¨Ø¹ Ù†Ø³Ø®Ù‡ 10 Ø¬ÙˆÙ„Ø§ÛŒ

from extensions import db
from models import HistoricalData, ComprehensiveSymbolData, TechnicalIndicatorData, FundamentalData, CandlestickPatternDetection
from flask import current_app
from sqlalchemy import func, distinct, text
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.exc import SQLAlchemyError, OperationalError
from sqlalchemy.dialects.sqlite import insert
from sqlalchemy import or_ # Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ø§ÛŒÙ† Ø®Ø· import Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
from datetime import datetime, date, timedelta
import jdatetime
import pandas as pd
import ta
import numpy as np
import requests
from bs4 import BeautifulSoup
import lxml
import time
import gc
import psutil
import logging
import re
from typing import Dict, List, Optional, Tuple, Any, Union
import concurrent.futures
import pytse_client
import traceback

from utils import calculate_stochastic, calculate_squeeze_momentum, calculate_halftrend, calculate_support_resistance_break, check_candlestick_patterns
from sqlalchemy.orm import aliased

import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ----------------------------
# Session maker
# ----------------------------
def get_session_local():
    """Ø§ÛŒØ¬Ø§Ø¯ session local Ø¨Ø§ application context"""
    try:
        from flask import current_app
        with current_app.app_context():
            return sessionmaker(bind=db.engine)()
    except RuntimeError:
        # Ø§Ú¯Ø± Ø®Ø§Ø±Ø¬ Ø§Ø² application context Ù‡Ø³ØªÛŒÙ…
        return sessionmaker(bind=db.get_engine())()

# ----------------------------
# Market type mappings
# ----------------------------
MARKET_TYPE_MAP = {
    '1': 'Ø¨ÙˆØ±Ø³',
    '2': 'ÙØ±Ø§Ø¨ÙˆØ±Ø³',
    '3': 'Ø¨ÙˆØ±Ø³ Ú©Ø§Ù„Ø§',
    '4': 'Ø¨ÙˆØ±Ø³ Ø§Ù†Ø±Ú˜ÛŒ',
    '5': 'ØµÙ†Ø¯ÙˆÙ‚ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ',
    '6': 'Ø§ÙˆØ±Ø§Ù‚ Ø¨Ø§ Ø¯Ø±Ø¢Ù…Ø¯ Ø«Ø§Ø¨Øª',
    '7': 'Ù…Ø´ØªÙ‚Ù‡',
    '8': 'Ø¹Ù…ÙˆÙ…ÛŒ',
    '9': 'Ù¾Ø§ÛŒÙ‡ ÙØ±Ø§Ø¨ÙˆØ±Ø³',
    '10': 'Ø§ÙˆØ±Ø§Ù‚ ØªØ§Ù…ÛŒÙ† Ù…Ø§Ù„ÛŒ',
    '11': 'Ø§ÙˆØ±Ø§Ù‚ Ø¨Ø§ Ø¯Ø±Ø¢Ù…Ø¯ Ø«Ø§Ø¨Øª',
    '12': '-'
}

HTML_MARKET_TYPE_MAP = {
    'Ø¨ÙˆØ±Ø³ Ø§ÙˆØ±Ø§Ù‚ Ø¨Ù‡Ø§Ø¯Ø§Ø± ØªÙ‡Ø±Ø§Ù†': 'Ø¨ÙˆØ±Ø³',
    'ÙØ±Ø§Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†': 'ÙØ±Ø§Ø¨ÙˆØ±Ø³',
    'Ø¨ÙˆØ±Ø³ Ú©Ø§Ù„Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†': 'Ø¨ÙˆØ±Ø³ Ú©Ø§Ù„Ø§',
    'Ø¨ÙˆØ±Ø³ Ø§Ù†Ø±Ú˜ÛŒ Ø§ÛŒØ±Ø§Ù†': 'Ø¨ÙˆØ±Ø³ Ø§Ù†Ø±Ú˜ÛŒ',
    'ØµÙ†Ø¯ÙˆÙ‚ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ': 'ØµÙ†Ø¯ÙˆÙ‚ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ',
    'Ø¨Ø§Ø²Ø§Ø± Ù¾Ø§ÛŒÙ‡ ÙØ±Ø§Ø¨ÙˆØ±Ø³': 'Ù¾Ø§ÛŒÙ‡ ÙØ±Ø§Ø¨ÙˆØ±Ø³',
    'Ø§ÙˆØ±Ø§Ù‚ Ø¨Ø§ Ø¯Ø±Ø¢Ù…Ø¯ Ø«Ø§Ø¨Øª': 'Ø§ÙˆØ±Ø§Ù‚ Ø¨Ø§ Ø¯Ø±Ø¢Ù…Ø¯ Ø«Ø§Ø¨Øª',
    'Ø§ÙˆØ±Ø§Ù‚ ØªØ§Ù…ÛŒÙ† Ù…Ø§Ù„ÛŒ': 'Ø§ÙˆØ±Ø§Ù‚ ØªØ§Ù…ÛŒÙ† Ù…Ø§Ù„ÛŒ'
}

# ----------------------------
# Filter configuration
# ----------------------------
BAD_SUFFIXES = ('Ø­', 'Ø¶', 'Øµ', 'Ùˆ')  # Ø­Ù‚ ØªÙ‚Ø¯Ù… Ùˆ Ù…Ø´Ø§Ø¨Ù‡

VALID_MARKET_KEYWORDS = list(set(list(MARKET_TYPE_MAP.values()) + list(HTML_MARKET_TYPE_MAP.values())))  # e.g., ['Ø¨ÙˆØ±Ø³', 'ÙØ±Ø§Ø¨ÙˆØ±Ø³', 'Ø¨ÙˆØ±Ø³ Ú©Ø§Ù„Ø§', ...]

INVALID_MARKET_KEYWORDS = [
    'Ø§Ø®ØªÛŒØ§Ø±',   # Ø§Ø®ØªÛŒØ§Ø± Ù…Ø¹Ø§Ù…Ù„Ù‡
    'Ø¢ØªÛŒ',      # Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯ Ø¢ØªÛŒ
    'Ù…Ø´ØªÙ‚Ù‡',    # Ø¨Ø§Ø²Ø§Ø± Ù…Ø´ØªÙ‚Ù‡
    'ØªØ³Ù‡ÛŒÙ„Ø§Øª'   # Ø§ÙˆØ±Ø§Ù‚ ØªØ³Ù‡ÛŒÙ„Ø§Øª Ù…Ø³Ú©Ù†
]

# ----------------------------
# Unified filter
# ----------------------------
def is_symbol_valid(symbol_name: str, market_type_name: str) -> bool:
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ù†Ù…Ø§Ø¯ (Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ticker Ùˆ Ù‡Ù… Ø¨Ø±Ø§ÛŒ dict).
    """
    try:
        if not symbol_name:
            return False

        # Ú¯Ø§Ù… Û±: ÙÛŒÙ„ØªØ± Ø­Ù‚ ØªÙ‚Ø¯Ù…â€ŒÙ‡Ø§ (Ø¨Ù‡Ø¨ÙˆØ¯ regex: ÙÙ‚Ø· Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ ÛŒØ§ Ø¹Ø¨Ø§Ø±Øª Ú©Ø§Ù…Ù„)
        if (symbol_name.endswith(BAD_SUFFIXES) and len(symbol_name) > 1) or re.search(r"\b(Ø­Ù‚\s*ØªÙ‚Ø¯Ù…|Ø­\.?\s*ØªÙ‚Ø¯Ù…)\b", symbol_name, re.IGNORECASE):
            logger.debug(f"ÙÛŒÙ„ØªØ± Ø­Ù‚ ØªÙ‚Ø¯Ù…: {symbol_name}")
            return False

        # Ú¯Ø§Ù… Û³: Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Ø²Ø§Ø± Ù†Ø§Ù…Ø¹ØªØ¨Ø± (blacklist - Ù‡Ù…ÛŒØ´Ù‡ Ø§Ø¹Ù…Ø§Ù„ Ú©Ù†)
        market_lower = market_type_name.lower() if market_type_name else ''
        if any(keyword.lower() in market_lower for keyword in INVALID_MARKET_KEYWORDS):
            logger.debug(f"ÙÛŒÙ„ØªØ± INVALID: {symbol_name} ({market_type_name})")
            return False

        # Ú¯Ø§Ù… Û²: Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Ø²Ø§Ø± Ù…Ø¹ØªØ¨Ø± (whitelist - ÙÙ‚Ø· Ø§Ú¯Ø± market_type_name ØºÛŒØ±Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ù‡)
        if market_type_name:  # Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ù‡ØŒ ÙØ±Ø¶ Ù…Ø¹ØªØ¨Ø±
            market_lower = market_type_name.lower()
            if not any(keyword.lower() in market_lower for keyword in VALID_MARKET_KEYWORDS):
                logger.debug(f"ÙÛŒÙ„ØªØ± VALID fail: {symbol_name} ({market_type_name})")  # log Ø¨Ø±Ø§ÛŒ debug
                return False

        return True

    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙÛŒÙ„ØªØ± Ù†Ù…Ø§Ø¯ {symbol_name}: {e}")
        return False  # safe fallback: ÙÛŒÙ„ØªØ± Ù†Ú©Ù† Ø§Ú¯Ø± exception

# ----------------------------
# Ù†Ø³Ø®Ù‡ dict-based Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§
# ----------------------------
def filter_symbols(symbols: List[Dict]) -> List[Dict]:
    """Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ù…Ø±Ú©Ø²ÛŒ Ø±ÙˆÛŒ Ù„ÛŒØ³Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§"""
    if not symbols:
        return []

    filtered = []
    filtered_count = 0
    for symbol in symbols:
        symbol_name = symbol.get('symbol_name', '')
        market_type_code = symbol.get('market_type_code')
        market_type_name = MARKET_TYPE_MAP.get(market_type_code, '')  # Ø§Ø² MAP Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†

        if is_symbol_valid(symbol_name, market_type_name):
            filtered.append(symbol)
            filtered_count += 1

    logger.info(f"ÙÛŒÙ„ØªØ± Ù†Ù…Ø§Ø¯Ù‡Ø§: Ø§Ø² {len(symbols)}ØŒ {filtered_count} Ù…Ø¹ØªØ¨Ø± (ÙÛŒÙ„ØªØ±Ø´Ø¯Ù‡: {len(symbols) - filtered_count})")
    return filtered


# ----------------------------
# Global defaults and constants
# ----------------------------
# Network
DEFAULT_REQUESTS_TIMEOUT = 8  # seconds
DEFAULT_RETRY_TOTAL = 3
DEFAULT_RETRY_BACKOFF_FACTOR = 0.5
DEFAULT_RETRY_STATUS_FORCELIST = (429, 500, 502, 503, 504)

# Batching / memory
DEFAULT_BATCH_SIZE = 200  # for DB bulk ops & symbol processing
DEFAULT_PER_SYMBOL_DELAY = 0.03  # polite delay between external website hits
MEMORY_CHECK_INTERVAL = 10  # check memory every N symbols during full runs (configurable)
MEMORY_LIMIT_MB = 1500  # warn threshold

# Misc
ZERO_WIDTH_CHARS = ["\u200c", "\u200b", "\ufeff"]  # remove zero width and BOM
HALF_SPACE = "\u200f"  # any specific characters you'd like to normalize

# ----------------------------
# Utilities
# ----------------------------
def normalize_symbol_text(txt: Optional[str]) -> str:
    """Normalize symbol / company names: strip, remove zero-width, collapse spaces."""
    if not txt:
        return ""
    s = str(txt)
    for ch in ZERO_WIDTH_CHARS:
        s = s.replace(ch, "")
    # replace multiple whitespace with single space and strip
    s = " ".join(s.split())
    return s.strip()

def check_memory_usage_mb() -> float:
    """Return current process memory usage in MB (if psutil available)."""
    try:
        if psutil:
            proc = psutil.Process()
            mem = proc.memory_info().rss / (1024 * 1024)
            return mem
        else:
            # fallback: attempt approximate using resource module (Unix only) or 0
            return 0.0
    except Exception as e:
        logger.debug("Memory check failed: %s", e)
        return 0.0

def safe_sleep(seconds: float):
    """Sleep but allow KeyboardInterrupt to bubble up quickly."""
    try:
        time.sleep(seconds)
    except KeyboardInterrupt:
        raise

# ----------------------------
# Retry decorator (general)
# ----------------------------
def retry_on_exception(
    exceptions: Tuple[type, ...] = (Exception,),
    tries: int = 3,
    delay: float = 0.5,
    backoff: float = 2.0,
    logger_obj: Optional[logging.Logger] = None,
):
    """
    Generic retry decorator with exponential backoff.
    Usage:
        @retry_on_exception((requests.RequestException,), tries=3)
        def fn(...): ...
    """
    def deco_retry(func):
        def wrapper(*args, **kwargs):
            _tries, _delay = tries, delay
            while _tries > 1:
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    msg = f"Retryable error in {func.__name__}: {e}. Retrying in {_delay}s..."
                    if logger_obj:
                        logger_obj.warning(msg)
                    else:
                        logger.warning(msg)
                    time.sleep(_delay)
                    _tries -= 1
                    _delay *= backoff
            # final attempt
            return func(*args, **kwargs)
        wrapper.__name__ = func.__name__
        return wrapper
    return deco_retry

# ----------------------------
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Ø§ÛŒØ¬Ø§Ø¯ session Ø¨Ø±Ø§ÛŒ requests
def create_requests_session():
    """Create a requests session with retry logic"""
    session = requests.Session()
    
    # Retry configuration
    retry_strategy = Retry(
        total=3,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # Set default headers
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
    })
    
    return session

_requests_session = create_requests_session()

# ----------------------------
# Wrapper: pytse-client safe adapter (primary)
# ----------------------------

try:
    import pytse_client as tse
    from pytse_client import Ticker, download, config
except ImportError:
    tse = None
    Ticker = None
    download = None
    config = None
    logger.warning("pytse_client not available")

class PytseClientWrapper:
    """
    Safe wrapper around pytse_client (primary source).
    Provides:
      - all_symbols()
      - ticker(symbol)
      - fetch_historical(symbol_id, start_date, end_date)
    Behavior:
      - If pytse-client is not importable or throws errors, this wrapper raises its own exceptions
        so caller can fallback to InternalTseWrapper.
    """
    def __init__(self, per_symbol_delay: float = DEFAULT_PER_SYMBOL_DELAY):
        self.per_symbol_delay = per_symbol_delay
        self.tse = tse  # may be None
        if self.tse is None:
            logger.info("pytse_client not available at import time.")
        # local small cache to reduce repeated ticker creations in same run
        self._ticker_cache: Dict[str, Any] = {}
        # requests session reuse for any direct HTTP calls if needed
        self.session = _requests_session

    def is_available(self):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØ§ pytse_client Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª"""
        return self.tse is not None

    def available(self) -> bool:
        """Ù…ØªØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ"""
        return self.is_available()

    @retry_on_exception((Exception,), tries=2, delay=0.5, backoff=2.0, logger_obj=logger)
    def all_symbols(self) -> List[str]:
        """Return list of symbol ids (or keys) from pytse-client"""
        if not self.is_available():
            raise RuntimeError("pytse_client not available")
        try:
            result = self.tse.all_symbols()
            # some versions return dict of id->meta
            if isinstance(result, dict):
                return list(result.keys())
            if isinstance(result, (list, tuple, set)):
                return list(result)
            # fallback: try to coerce
            return [str(x) for x in result]
        except Exception as e:
            logger.exception("pytse_client.all_symbols() failed: %s", e)
            raise

    def get_all_symbols(self):
        """Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù…ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ - Ù…ØªØ¯ Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Ú©Ø¯ Ù…ÙˆØ¬ÙˆØ¯"""
        if not self.is_available():
            return {}
        try:
            tickers = download(symbols="all", write_to_csv=False)
            return tickers
        except Exception as e:
            logger.error(f"Error getting all symbols: {e}")
            return {}

    def ticker(self, symbol_id: str):
        """Return Ticker object from pytse_client, cached"""
        if not self.is_available():
            raise RuntimeError("pytse_client not available")
        key = str(symbol_id)
        if key in self._ticker_cache:
            return self._ticker_cache[key]
        try:
            ticker = Ticker(symbol=key)
            self._ticker_cache[key] = ticker
            # polite delay
            if self.per_symbol_delay:
                safe_sleep(self.per_symbol_delay)
            return ticker
        except Exception:
            logger.exception("Failed to get Ticker for %s", symbol_id)
            raise

    def get_ticker(self, symbol_name):
        """Ø§ÛŒØ¬Ø§Ø¯ Ticker object Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ - Ù†Ø³Ø®Ù‡ Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Ú©Ø¯ Ù…ÙˆØ¬ÙˆØ¯"""
        if not self.is_available():
            return None
        try:
            ticker = Ticker(symbol_name)
            return ticker
        except Exception as e:
            logger.error(f"Error creating Ticker for {symbol_name}: {e}")
            return None

    @retry_on_exception((Exception,), tries=2, delay=1.0, backoff=2.0, logger_obj=logger)
    def fetch_historical(self, symbol_id: str, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> Any:
        """
        Fetch historical data for symbol using pytse-client high-level APIs if available.
        Return types may vary by pytse-client version: pandas.DataFrame OR dict OR text.
        Caller should handle/normalize.
        """
        if not self.is_available():
            raise RuntimeError("pytse_client not available")

        try:
            # many pytse-client versions expose a function to download history. We try common patterns.
            # 1) tse.download... or tse.get_historical or Ticker().history
            if hasattr(self.tse, "download") or hasattr(self.tse, "history"):
                # attempt Ticker.history first if available
                try:
                    t = self.ticker(symbol_id)
                    if hasattr(t, "history"):
                        hist = getattr(t, "history")()
                        return hist
                except Exception:
                    pass

            # Fallback generic APIs (some versions implement get_history or download_from)
            if hasattr(self.tse, "get_history"):
                return self.tse.get_history(symbol_id, start_date=start_date, end_date=end_date)
            if hasattr(self.tse, "download"):
                # typical signature: tse.download(symbol_id, start, end)
                try:
                    return self.tse.download(symbol_id, start=start_date, end=end_date)
                except TypeError:
                    # try alternative signature
                    return self.tse.download(symbol_id)
            # Last resort: try calling tse.Ticker and reading attributes or methods
            t = self.ticker(symbol_id)
            if hasattr(t, "history"):
                return t.history()
            # If nothing works, raise
            raise RuntimeError("No supported historical API in installed pytse_client")
        except Exception as e:
            logger.exception("pytse-client historical fetch failed for %s: %s", symbol_id, e)
            raise

# ----------------------------
# Internal fallback wrapper (if pytse-client fails)
# ----------------------------

class InternalTseWrapper:
    """
    Fallback methods that use TSETMC endpoints directly (HTTP scraping/JSON endpoints).
    This is slower and more brittle, but necessary as a fallback.
    Key functions:
      - search_symbol_by_name(name)
      - fetch_instrument_info_by_id(instrument_id)
      - fetch_export_txt(...)
      - parse loader page to infer market/group etc.
    NOTE: Keep polite delays and caching here to avoid hammering TSETMC.
    """
    def __init__(self, per_call_delay: float = DEFAULT_PER_SYMBOL_DELAY):
        self.session = _requests_session
        self.per_call_delay = per_call_delay
        self._simple_cache: Dict[str, Any] = {}

    def _get(self, url: str, params: Optional[Dict[str, Any]] = None, timeout: Optional[int] = None) -> requests.Response:
        try:
            resp = self.session.get(url, params=params, timeout=timeout or DEFAULT_REQUESTS_TIMEOUT)
            return resp
        except Exception:
            logger.exception("HTTP GET failed for %s", url)
            raise

    def search_symbol_by_name(self, query: str) -> List[Dict[str, Any]]:
        """
        Use old.tsetmc.com search endpoint to find instrument IDs by textual name.
        Returns a list of dicts (best-effort) with keys like 'id', 'symbol', 'title'
        """
        q = normalize_symbol_text(query)
        cache_key = f"search:{q}"
        if cache_key in self._simple_cache:
            return self._simple_cache[cache_key]

        try:
            url = "https://old.tsetmc.com/tsev2/data/search.aspx"
            params = {"skey": q}
            resp = self._get(url, params=params)
            if resp.status_code != 200:
                logger.debug("search endpoint returned status %s for %s", resp.status_code, q)
                return []
            text = resp.text or ""
            # The search endpoint often returns a small text: small html or some csv-like
            # try to parse ids from text heuristically
            results = []
            # simple heuristics: split lines and find parenthesis with id or pattern 'i=...'
            for line in text.splitlines():
                line = line.strip()
                if not line:
                    continue
                # example pattern: "some name (symbol) - <a href='...i=1234567'>"
                # fallback: look for sequences of digits length>6
                import re
                m = re.search(r"i=(\d{6,})", line)
                if m:
                    iid = m.group(1)
                    results.append({"id": iid, "raw": line})
            # fallback more: if we didn't parse anything, try to extract trailing tokens
            if not results:
                # try whitespace tokenization for any long numeric tokens
                import re
                for token in re.findall(r"\d{6,}", text):
                    results.append({"id": token, "raw": token})
            self._simple_cache[cache_key] = results
            safe_sleep(self.per_call_delay)
            return results
        except Exception:
            logger.exception("search_symbol_by_name failed for query: %s", query)
            return []

    def fetch_export_txt(self, instrument_id: str) -> Optional[str]:
        """
        Call Export-txt.aspx to get instrument details text
        """
        try:
            url = f"https://old.tsetmc.com/tsev2/data/Export-txt.aspx"
            params = {"t": "i", "a": 1, "b": 0, "i": instrument_id}
            resp = self._get(url, params=params)
            if resp.status_code != 200:
                logger.debug("export-txt returned %s for %s", resp.status_code, instrument_id)
                return None
            safe_sleep(self.per_call_delay)
            return resp.text
        except Exception:
            logger.exception("fetch_export_txt failed for %s", instrument_id)
            return None

    def fetch_instrument_info_api(self, instrument_id: str) -> Optional[Dict[str, Any]]:
        """Call cdn.tsetmc.com/api/Instrument/GetInstrumentInfo/{id}"""
        try:
            url = f"https://cdn.tsetmc.com/api/Instrument/GetInstrumentInfo/{instrument_id}"
            resp = self._get(url)
            if resp.status_code != 200:
                return None
            data = resp.json()
            if not data or not isinstance(data, dict):
                return None
            # extract useful fields
            result = {}
            # example structure: {"instrumentInfo": { ... }, ... }
            # we try to extract common fields
            if "instrumentInfo" in data:
                info = data["instrumentInfo"]
                if isinstance(info, dict):
                    result["name"] = info.get("lVal18AFC")
                    result["symbol"] = info.get("lVal18")
                    result["company_name"] = info.get("lVal30")
                    result["market"] = info.get("market")
                    result["industry"] = info.get("sector")
            safe_sleep(self.per_call_delay)
            return result
        except Exception:
            logger.exception("instrument info API failed for %s", instrument_id)
            return None

    def fetch_loader_page(self, instrument_id: str) -> Optional[str]:
        """Fetch loader.aspx page to parse market/group info from HTML"""
        try:
            url = f"https://old.tsetmc.com/Loader.aspx"
            params = {"i": instrument_id, "Partree": "15131M"}
            resp = self._get(url, params=params)
            if resp.status_code != 200:
                return None
            safe_sleep(self.per_call_delay)
            return resp.text
        except Exception:
            logger.exception("loader page fetch failed for %s", instrument_id)
            return None

    def parse_loader_page(self, html: str) -> Dict[str, Any]:
        """Parse loader.aspx HTML to extract market/group info"""
        result = {}
        if not html:
            return result
        try:
            soup = BeautifulSoup(html, "lxml")
            # look for breadcrumbs or specific table rows
            # example: <a href="...">Ø¨Ø§Ø²Ø§Ø± Ø§ÙˆÙ„</a> or <td>Ø¨Ø§Ø²Ø§Ø±</td><td>Ø¨Ø§Ø²Ø§Ø± Ø§ÙˆÙ„</td>
            # try to find common patterns
            # pattern 1: breadcrumb links
            links = soup.find_all("a")
            for link in links:
                href = link.get("href", "")
                text = normalize_symbol_text(link.get_text())
                if "Partree" in href and text:
                    # example href: "?Partree=15131P&i=1234567"
                    # text might be like "Ø¨Ø§Ø²Ø§Ø± Ø§ÙˆÙ„", "Ø¨Ø§Ø²Ø§Ø± Ø¯ÙˆÙ…", etc.
                    result["market"] = text
            # pattern 2: table rows with key-value
            rows = soup.find_all("tr")
            for row in rows:
                tds = row.find_all("td")
                if len(tds) >= 2:
                    key = normalize_symbol_text(tds[0].get_text())
                    value = normalize_symbol_text(tds[1].get_text())
                    if key and value:
                        # common keys: "Ø¨Ø§Ø²Ø§Ø±", "Ú¯Ø±ÙˆÙ‡ ØµÙ†Ø¹Øª", "Ù†Ù…Ø§Ø¯", "Ù†Ø§Ù… Ø´Ø±Ú©Øª"
                        if "Ø¨Ø§Ø²Ø§Ø±" in key:
                            result["market"] = value
                        elif "Ú¯Ø±ÙˆÙ‡" in key:
                            result["industry"] = value
                        elif "Ù†Ø§Ù… Ø´Ø±Ú©Øª" in key:
                            result["company_name"] = value
                        elif "Ù†Ù…Ø§Ø¯" in key:
                            result["symbol"] = value
            return result
        except Exception:
            logger.exception("parse_loader_page failed")
            return result

    def get_instrument_details(self, instrument_id: str) -> Dict[str, Any]:
        """Combined method to get details for an instrument ID using multiple fallbacks"""
        result = {}
        # try API first
        api_info = self.fetch_instrument_info_api(instrument_id)
        if api_info:
            result.update(api_info)
        # then try loader page
        html = self.fetch_loader_page(instrument_id)
        if html:
            parsed = self.parse_loader_page(html)
            result.update(parsed)
        # then try export-txt
        txt = self.fetch_export_txt(instrument_id)
        if txt:
            # simple parsing: split lines and look for patterns
            lines = txt.splitlines()
            for line in lines:
                line = line.strip()
                if ":" in line:
                    parts = line.split(":", 1)
                    key = parts[0].strip()
                    value = parts[1].strip() if len(parts) > 1 else ""
                    # map known keys
                    if key == "Ù†Ù…Ø§Ø¯":
                        result["symbol"] = value
                    elif key == "Ù†Ø§Ù… Ø´Ø±Ú©Øª":
                        result["company_name"] = value
                    elif key == "Ø¨Ø§Ø²Ø§Ø±":
                        result["market"] = value
                    elif key == "Ú¯Ø±ÙˆÙ‡ ØµÙ†Ø¹Øª":
                        result["industry"] = value
        return result

# ----------------------------
# Global instances
# ----------------------------
pytse_wrapper = PytseClientWrapper()
internal_wrapper = InternalTseWrapper()


# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
# ----------------------------
def setup_robust_session(retries=5, backoff_factor=2, timeout=60):
    """ØªÙ†Ø¸ÛŒÙ… Ø³Ø´Ù† Ø¨Ø§ Retry Ùˆ Timeout Ø¨Ø§Ù„Ø§"""
    # Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ùˆ Ø¶Ø±ÛŒØ¨ ØªØ§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ù†Ø§Ù¾Ø§ÛŒØ¯Ø§Ø±
    retry_strategy = Retry(
        total=retries,
        backoff_factor=backoff_factor, # ØªØ§Ø®ÛŒØ±: 2s, 4s, 8s, 16s...
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    http = requests.Session()
    http.mount("https://", adapter)
    http.mount("http://", adapter)
    # ØªÙ†Ø¸ÛŒÙ… Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ Ø³Ø´Ù† (Ø§Ø®ØªÛŒØ§Ø±ÛŒØŒ Ø§Ù…Ø§ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
    http.timeout = timeout
    return http

# ----------------------------
# ØªØ§Ø¨Ø¹ Ù„ÛŒØ³Øª Ú©Ø±Ø¯Ù† Ø³Ù‡Ù…â€ŒÙ‡Ø§ (populate_comprehensive_symbols)
# ----------------------------
def populate_comprehensive_symbols(db_session, batch_size: int = 200):
    """
    Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø² pytse-client Ùˆ Ø¯Ø±Ø¬/Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø± ComprehensiveSymbolData Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ.
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ú©Ù†ÙˆÙ† Retry Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÙØ§Ø² Ø¯Ø±ÛŒØ§ÙØª Ø§ÙˆÙ„ÛŒÙ‡ Ùˆ ÙØ§Ø² ØªÛŒÚ©Ø±ÛŒÙ†Ú¯ (Ticker) Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    
    logger.info(f"ğŸ“¥ Ø´Ø±ÙˆØ¹ Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¯Ø± ComprehensiveSymbolData Ø¨Ø§ Ø¨Ú†â€ŒØ³Ø§ÛŒØ² {batch_size}...")

    try:
        import pytse_client as tse
        
        # 1. ÙØ§Ø² Ø§ÙˆÙ„ÛŒÙ‡: Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§ (Ø¨Ø§ Retry Ø¯Ø± Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§)
        
        # ğŸ’¡ Ø§ÛŒØ¬Ø§Ø¯ Ø³Ø´Ù† Ù‚ÙˆÛŒ Ø¨Ø±Ø§ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ download (Ø§ÛŒÙ† Ø³Ø´Ù† Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ù…Ø§ Ø§Ù„Ú¯ÙˆÛŒ Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª)
        # robust_http_session = setup_robust_session(retries=5, backoff_factor=2, timeout=120) 
        
        all_tickers = None
        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ 3 Ø¨Ø§Ø± Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ú©Ù„ Ù†Ù…Ø§Ø¯Ù‡Ø§
        for attempt in range(1, 4):
            try:
                logger.info(f"ğŸš€ ØªÙ„Ø§Ø´ {attempt} Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ú©Ù„ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø² TSETMC...")
                
                # âŒ Ù¾Ø§Ø±Ø§Ù…ØªØ± 'timeout' Ø­Ø°Ù Ø´Ø¯ØŒ Ø²ÛŒØ±Ø§ pytse_client Ø§Ø² Ø¢Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
                all_tickers = tse.download(symbols="all", write_to_csv=False) 
                break # Ø§Ú¯Ø± Ù…ÙˆÙÙ‚ Ø´Ø¯ØŒ Ø­Ù„Ù‚Ù‡ Ø±Ø§ Ù…ÛŒâ€ŒØ´Ú©Ù†ÛŒÙ…
            except (requests.exceptions.Timeout, urllib3.exceptions.ReadTimeoutError, TypeError, Exception) as e:
                # ğŸ’¡ TypeError Ø±Ø§ Ø¨Ù‡ Ù„ÛŒØ³Øª Ø§Ø³ØªØ«Ù†Ø§Ù‡Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯ÛŒÙ… ØªØ§ Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø®Ø·Ø§ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø¯Ø§Ø¯ØŒ Ù„Ø§Ú¯ Ø´ÙˆØ¯.
                if isinstance(e, TypeError) and "unexpected keyword argument 'timeout'" in str(e):
                    logger.error("âŒ Ø®Ø·Ø§ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±: ØªØ§Ø¨Ø¹ tse.download Ø§Ø² 'timeout' Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¨Ù‡ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ ØªÚ©ÛŒÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….")
                    # Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ù¾Ø§Ø±Ø§Ù…ØªØ± 'timeout' Ú©Ù‡ Ø¨Ø§Ø¹Ø« Ø®Ø·Ø§ Ø´Ø¯Ù‡ Ø­Ø°Ù Ø´Ø¯Ù‡ Ø§Ø³Øª. 
                    # Ø§Ú¯Ø± Ø§ÛŒÙ† Ø®Ø·Ø§ Ù…Ø¬Ø¯Ø¯Ø§Ù‹ Ø±Ø® Ø¯Ù‡Ø¯ (Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ø¨Ø¯Ù‡Ø¯)ØŒ Ø´Ø§ÛŒØ¯ Ù…Ø´Ú©Ù„ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ø§Ø´Ø¯.
                else:
                    logger.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ ÛŒØ§ Ø´Ø¨Ú©Ù‡ Ø¯Ø± ØªÙ„Ø§Ø´ {attempt} Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ú©Ù„ Ù†Ù…Ø§Ø¯Ù‡Ø§: {e}")
                    
                if attempt == 3:
                    raise Exception(f"âŒ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ú©Ù„ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ù¾Ø³ Ø§Ø² Û³ ØªÙ„Ø§Ø´: {e}") from e
                time.sleep(10 * attempt) # ØªØ§Ø®ÛŒØ± ØªØµØ§Ø¹Ø¯ÛŒ Ù‚Ø¨Ù„ Ø§Ø² ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯

        if not all_tickers:
             return {"added": 0, "updated": 0, "message": "Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯."}


        # 2. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ú†â€ŒØ¨Ù†Ø¯ÛŒ
        all_symbol_names = list(all_tickers.keys())
        total_symbols = len(all_symbol_names)
        logger.info(f"âœ… Ù„ÛŒØ³Øª Ø§ÙˆÙ„ÛŒÙ‡ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹ Ù†Ù…Ø§Ø¯Ù‡Ø§: {total_symbols}")
        
        added_count = 0
        updated_count = 0
        now = datetime.now()
        
        # 3. Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        for i in range(0, total_symbols, batch_size):
            batch_symbol_names = all_symbol_names[i:i + batch_size]
            logger.info(f"--- Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡ {i//batch_size + 1} Ø§Ø² {len(batch_symbol_names)} Ù†Ù…Ø§Ø¯ ---")
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆÙ‚Øª Ø¯Ø± Ù‡Ø± Ø¨Ú†
            batch_records_to_commit = []

            for symbol_name in batch_symbol_names:
                
                # 3.1. ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ (Ø¨Ø± Ø§Ø³Ø§Ø³ flow Ú©Ù‡ Ø¯Ø± tse.download Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª)
                ticker_data = all_tickers.get(symbol_name, {})
                flow = ticker_data.get('flow', '')
                # âš ï¸ ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ is_symbol_valid Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª
                if not is_symbol_valid(symbol_name, flow):
                    logger.debug(f"â© Ù†Ù…Ø§Ø¯ {symbol_name} ÙÛŒÙ„ØªØ± Ø´Ø¯ (flow: {flow})")
                    continue

                # 3.2. ÙØ§Ø² Ø¬Ø²Ø¦ÛŒØ§Øª: Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ticker (Ø¨Ø§ Retry Ø¯Ø± Ø³Ø·Ø­ Ù†Ù…Ø§Ø¯)
                ticker = None
                for retry in range(1, 4):
                    try:
                        ticker = tse.Ticker(symbol_name)
                        break
                    except Exception as e:
                        logger.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ù†Ù…Ø§Ø¯ {symbol_name} (ØªÙ„Ø§Ø´ {retry}): {e}")
                        if retry == 3:
                            logger.error(f"âŒ Ù†Ù…Ø§Ø¯ {symbol_name} Ù¾Ø³ Ø§Ø² Û³ ØªÙ„Ø§Ø´ Skip Ø´Ø¯.")
                            ticker = None
                            break
                        time.sleep(5 * retry) # ØªØ§Ø®ÛŒØ± Ù‚Ø¨Ù„ Ø§Ø² ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯
                
                if ticker is None:
                    continue # Ø§Ú¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯ØŒ Ø¨Ù‡ Ù†Ù…Ø§Ø¯ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ùˆ

                # 3.3. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
                try:
                    
                    if not is_symbol_valid(symbol_name, getattr(ticker, 'flow', '')):
                        continue
                    
                    tse_index = getattr(ticker, 'index', None)
                    if not tse_index:
                        logger.warning(f"âš ï¸ Ù†Ù…Ø§Ø¯ {symbol_name} Ø´Ù†Ø§Ø³Ù‡ Ø¨ÙˆØ±Ø³ Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                        continue
                    
                    base_volume = getattr(ticker, 'base_volume', None)
                    eps = getattr(ticker, 'eps', None)
                    p_e_ratio = getattr(ticker, 'p_e_ratio', None)
                    market_cap = getattr(ticker, 'market_cap', None)
                    float_shares = getattr(ticker, 'float_shares', None)
                    fiscal_year = getattr(ticker, 'fiscal_year', None)
                    state = getattr(ticker, 'state', None)
                    p_s_ratio = getattr(ticker, 'p_s_ratio', None)
                    nav = getattr(ticker, 'nav', None)

                    if nav and isinstance(nav, str):
                        try:
                            nav = float(nav.replace(',', ''))
                        except ValueError:
                            nav = None

                    if nav is None and base_volume == 1:
                        logger.debug(f"â© Ù†Ù…Ø§Ø¯ {symbol_name} Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ nav=None Ùˆ base_volume=1 Ø±Ø¯ Ø´Ø¯.")
                        continue

                    # âš ï¸ ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ ComprehensiveSymbolData Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª
                    existing_symbol = db_session.query(ComprehensiveSymbolData).filter_by(
                        tse_index=tse_index
                    ).first()
                    
                    if existing_symbol:
                        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯
                        existing_symbol.symbol_name = symbol_name
                        existing_symbol.company_name = getattr(ticker, 'title', '')
                        existing_symbol.group_name = getattr(ticker, 'group_name', '')
                        existing_symbol.market_type = getattr(ticker, 'flow', '')
                        existing_symbol.base_volume = base_volume
                        existing_symbol.eps = eps
                        existing_symbol.p_e_ratio = p_e_ratio
                        existing_symbol.market_cap = market_cap
                        existing_symbol.float_shares = float_shares
                        existing_symbol.fiscal_year = fiscal_year
                        existing_symbol.state = state
                        existing_symbol.tse_index = tse_index
                        if p_s_ratio is not None:
                            existing_symbol.p_s_ratio = p_s_ratio
                        if nav is not None:
                            existing_symbol.nav = nav
                        existing_symbol.updated_at = now
                        updated_count += 1
                        batch_records_to_commit.append(existing_symbol)
                        
                    else:
                        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯
                        new_symbol = ComprehensiveSymbolData(
                            symbol_id=tse_index,
                            tse_index=tse_index,
                            symbol_name=symbol_name,
                            company_name=getattr(ticker, 'title', ''),
                            group_name=getattr(ticker, 'group_name', ''),
                            market_type=getattr(ticker, 'flow', ''),
                            base_volume=base_volume,
                            eps=eps,
                            p_e_ratio=p_e_ratio,
                            market_cap=market_cap,
                            float_shares=float_shares,
                            fiscal_year=fiscal_year,
                            state=state,
                            created_at=now,
                            updated_at=now
                        )
                        if p_s_ratio is not None:
                            new_symbol.p_s_ratio = p_s_ratio
                        if nav is not None:
                            new_symbol.nav = nav
                        
                        added_count += 1
                        batch_records_to_commit.append(new_symbol)
                        
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ú©ÙˆØ±Ø¯ Ù†Ù…Ø§Ø¯ {symbol_name}: {e}", exc_info=True)
                    # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ rollback Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ ÙÙ‚Ø· Ø§ÛŒÙ† Ù†Ù…Ø§Ø¯ Skip Ø´ÙˆØ¯ Ùˆ Ø¨Ú† Ø§Ø¯Ø§Ù…Ù‡ ÛŒØ§Ø¨Ø¯.
                    continue
            
            # 4. Commit Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù‡Ø± Ø¨Ú†
            try:
                for record in batch_records_to_commit:
                    db_session.add(record)
                db_session.commit()
                logger.info(f"âœ… Ø«Ø¨Øª Ø¯Ø³ØªÙ‡ {i//batch_size + 1} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³.")
                time.sleep(3) # Ù…Ú©Ø« Ú©ÙˆØªØ§Ù‡ Ø¨ÛŒÙ† Ø¨Ú†â€ŒÙ‡Ø§
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø«Ø¨Øª Ø¯Ø³ØªÙ‡ {i//batch_size + 1} Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}", exc_info=True)
                db_session.rollback()
                # Ø§Ú¯Ø± Commit Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯ØŒ Ú©Ù„ Ø¹Ù…Ù„ÛŒØ§Øª Ø±Ø§ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
                raise Exception(f"Ø®Ø·Ø§ Ø¯Ø± Ø«Ø¨Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¯Ø± Ø¯Ø³ØªÙ‡ {i//batch_size + 1}.") from e

        # 5. Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        logger.info(f"âœ… Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯. {added_count} Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ØŒ {updated_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯.")
        return {"added": added_count, "updated": updated_count}
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± populate_comprehensive_symbols: {e}", exc_info=True)
        db_session.rollback()
        # Ø§Ú¯Ø± Ø®Ø·Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø±Ø® Ø¯Ø§Ø¯ØŒ Ù¾ÛŒØ§Ù… Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ±ÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯.
        error_msg = str(e)
        if "Read timed out" in error_msg or "timeout" in error_msg.lower():
            # Ø§ÛŒÙ† Ø®Ø·Ø§ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¹Ø¯Ù… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù¾Ø§Ø±Ø§Ù…ØªØ± timeout Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ Ø§Ù…Ø§ Ù¾ÛŒØ§Ù… Ù…Ù†Ø§Ø³Ø¨ Ø±Ø§ Ø­ÙØ¸ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            error_msg = "Ø®Ø·Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ (Timeout) Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³Ø±ÙˆØ± Ø¨ÙˆØ±Ø³ Ø±Ø® Ø¯Ø§Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."
        raise Exception(error_msg) from e




# ----------------------------
# Â ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ (fetch_historical_and_fundamental_data)
# ----------------------------
def fetch_historical_and_fundamental_data(db_session: Session, batch_size: int = 200, limit: int = None):
    """
    Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯.

    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ (Batch) Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ÙˆØ§Ú©Ø´ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø³Ù¾Ø³
    Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù‡Ø± Ø¯Ø³ØªÙ‡ Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø§Ø² Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø²ÛŒØ§Ø¯
    Ø­Ø§ÙØ¸Ù‡ Ùˆ Ø®Ø·Ø§Ù‡Ø§ÛŒ Timeout Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø´ÙˆØ¯.
    
    Args:
        db_session: Ø³Ø´Ù† SQLAlchemy Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¨Ø§ Ø¯ÛŒØªØ§Ø¨ÛŒØ³.
        batch_size (int): ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¯Ø± Ù‡Ø± Ø¯Ø³ØªÙ‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 200).
        limit (int | None): Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆÙ†Ø¯.
    """
    try:
        logger.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø§ Ø¨Ú†â€ŒØ³Ø§ÛŒØ² {batch_size}...")

        # âš ï¸ ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ ComprehensiveSymbolDataØŒ fetch_and_process_historical_dataØŒ 
        # update_symbol_fundamental_data Ùˆ logger Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù‡Ø³ØªÙ†Ø¯.
        
        # 1. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø­Ù„Ù‚Ù‡
        offset = 0
        total_processed_count = 0
        total_historical_count = 0
        total_fundamental_count = 0
        
        # Ø´Ù…Ø§Ø±Ø´ Ú©Ù„ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯ Ø§ÙˆÙ„ÛŒÙ‡)
        symbols_count_query = db_session.query(ComprehensiveSymbolData).count()
        max_symbols_to_process = limit if limit is not None else symbols_count_query
        
        logger.info(f"ğŸ” Ù…Ø¬Ù…ÙˆØ¹ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {symbols_count_query}. Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´: {max_symbols_to_process}")

        # 2. Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        while total_processed_count < max_symbols_to_process:
            
            # ØªØ¹ÛŒÛŒÙ† limit Ø¨Ø±Ø§ÛŒ Ú©ÙˆØ¦Ø±ÛŒ ÙØ¹Ù„ÛŒ
            current_limit = min(batch_size, max_symbols_to_process - total_processed_count)

            # Ú©ÙˆØ¦Ø±ÛŒ Ú¯Ø±ÙØªÙ† id Ùˆ tse_index (Ø¨Ø§ Ø§Ø¹Ù…Ø§Ù„ offset Ùˆ limit)
            symbols_query = db_session.query(
                ComprehensiveSymbolData.id,
                ComprehensiveSymbolData.tse_index
            ).order_by(ComprehensiveSymbolData.id).offset(offset).limit(current_limit)
            
            symbols_batch = symbols_query.all()
            
            if not symbols_batch:
                # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ù†Ù…Ø§Ø¯ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú©Ø´ÛŒ Ù†Ø¨ÙˆØ¯ØŒ Ø­Ù„Ù‚Ù‡ Ø±Ø§ Ù…ÛŒâ€ŒØ´Ú©Ù†ÛŒÙ…
                break

            current_batch_size = len(symbols_batch)
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ tse_indexâ€ŒÙ‡Ø§ (Ú©Ù‡ Ú©Ù„ÛŒØ¯ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ticker Ø§Ø³Øª)
            batch_tse_indices = [tse_index for _, tse_index in symbols_batch]

            logger.info(f"--- Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡ {offset // batch_size + 1} Ø§Ø² {current_batch_size} Ù†Ù…Ø§Ø¯ (Offset: {offset}) ---")

            # 2.1. Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡ ÙØ¹Ù„ÛŒ
            try:
                # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø§ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø¨Ú† ÙØ¹Ù„ÛŒ
                processed_historical_count, historical_msg = fetch_and_process_historical_data(
                    db_session, 
                    specific_symbols_list=batch_tse_indices # Ø§Ø±Ø³Ø§Ù„ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø¨Ú† ÙØ¹Ù„ÛŒ
                )
                total_historical_count += processed_historical_count
                logger.info(f"ğŸ“ˆ [Ø¨Ú†] Ø¯Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ {len(symbols_batch)} Ù†Ù…Ø§Ø¯ Ù…ÙˆØ±Ø¯ Ø¨Ø±Ø±Ø³ÛŒ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØª.")
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª ØªØ§Ø±ÛŒØ®ÛŒ Ø¯Ø³ØªÙ‡ {offset // batch_size + 1}: {e}", exc_info=True)
            
            # 2.2. Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡ ÙØ¹Ù„ÛŒ
            batch_fundamental_count = 0
            try:
                # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø§ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø¨Ú† ÙØ¹Ù„ÛŒ
                updated_count, msg = update_symbol_fundamental_data(
                    db_session,
                    specific_symbols_list=batch_tse_indices
                )
                batch_fundamental_count += updated_count
                total_fundamental_count += batch_fundamental_count
                logger.info(f"ğŸ“Š [Ø¨Ú†] Ø¯Ø§Ø¯Ù‡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {batch_fundamental_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯.")
                
            except Exception as e:
                db_session.rollback()
                logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¯Ø³ØªÙ‡ {offset // batch_size + 1}: {e}", exc_info=True)
                
            
            # 2.3. Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Offset Ùˆ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡â€ŒÙ‡Ø§
            total_processed_count += current_batch_size
            offset += current_batch_size

            if offset >= max_symbols_to_process and limit is not None:
                break # Ø¨Ù‡ limit Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø³ÛŒØ¯ÛŒÙ…

        # 3. Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (Ù¾Ø³ Ø§Ø² Ø§ØªÙ…Ø§Ù… Ø­Ù„Ù‚Ù‡)
        success_msg = f"""
âœ… ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯:
â€¢ {total_historical_count} Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯ ØªØ§Ø±ÛŒØ®ÛŒ Ø«Ø¨Øª Ø´Ø¯.
â€¢ {total_fundamental_count} Ù†Ù…Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù†Ø¯
        """
        logger.info(success_msg)
        
        return {
            "historical": total_historical_count,
            "fundamental": total_fundamental_count,
            "message": success_msg
        }

    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {e}"
        logger.error(error_msg, exc_info=True)
        # ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ Rollback Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ… (Ø²ÛŒØ±Ø§ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ú† Ù‚Ø¨Ù„Ø§Ù‹ Rollback Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯)
        if db_session.is_active:
            db_session.rollback()
            
        return {
            "historical": 0,
            "fundamental": 0,
            "message": error_msg
        }




# ----------------------------
# ØªØ§Ø¨Ø¹ fetch_symbols_from_pytse_client (Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡ Ø¨Ø§ ÙÛŒÙ„ØªØ± Ù…Ø±Ú©Ø²ÛŒ)
# ----------------------------
def fetch_symbols_from_pytse_client(db_session: Session, limit: int = None):
    """
    Ú¯Ø±ÙØªÙ† Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø² pytse-client Ùˆ Ø¯Ø±Ø¬ Ø¯Ø± ComprehensiveSymbolData.
    """
    logger.info("ğŸ“¥ Ø´Ø±ÙˆØ¹ Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø² pytse_client Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø¬ Ø¯Ø± comprehensive_symbol_data...")

    try:
        import pytse_client as tse
        all_tickers = tse.download(symbols="all", write_to_csv=False)
        
        added_count = 0
        updated_count = 0
        now = datetime.now()

        for idx, (symbol_name, ticker_data) in enumerate(all_tickers.items()):
            if limit and idx >= limit:
                break

            # ÙÛŒÙ„ØªØ± Ø²ÙˆØ¯ØªØ± (Ù‚Ø¨Ù„ Ø§Ø² Ticker creation)
            # flow Ø±Ùˆ Ø§Ø² ticker_data Ø¨Ú¯ÛŒØ± Ø§Ú¯Ø± Ù…Ù…Ú©Ù†Ù‡ØŒ ÙˆÚ¯Ø±Ù†Ù‡ ''
            flow = ticker_data.get('flow', '') if isinstance(ticker_data, dict) else ''
            if not is_symbol_valid(symbol_name, flow):
                logger.debug(f"â© Ù†Ù…Ø§Ø¯ {symbol_name} ÙÛŒÙ„ØªØ± Ø´Ø¯ (flow: {flow})")
                continue  # Ø²ÙˆØ¯ØªØ± skip Ú©Ù†

            try:
                # Ø§ÛŒØ¬Ø§Ø¯ Ø´ÛŒ Ticker Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„
                ticker = tse.Ticker(symbol_name)
                
                # ğŸ’¡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÛŒÙ„ØªØ± Ù…Ø±Ú©Ø²ÛŒ (Ù†Ø³Ø®Ù‡ Ø¬Ø¯ÛŒØ¯)
                if not is_symbol_valid(symbol_name, getattr(ticker, 'flow', '')):
                    continue  # Ø§Ú¯Ø± Ù†Ù…Ø§Ø¯ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³ØªØŒ Ø¨Ø±Ùˆ Ø¨Ù‡ Ø¨Ø¹Ø¯ÛŒ
                
                # Ø¯Ø±ÛŒØ§ÙØª Ø´Ù†Ø§Ø³Ù‡ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ (index) Ø§Ø² Tsetmc
                tse_index = getattr(ticker, 'index', None)
                if not tse_index:
                    logger.warning(f"âš ï¸ Ù†Ù…Ø§Ø¯ {symbol_name} Ø´Ù†Ø§Ø³Ù‡ Ø¨ÙˆØ±Ø³ Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                    continue
                
                # ... Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯ ...
                base_volume = getattr(ticker, 'base_volume', None)
                eps = getattr(ticker, 'eps', None)
                p_e_ratio = getattr(ticker, 'p_e_ratio', None)
                market_cap = getattr(ticker, 'market_cap', None)
                float_shares = getattr(ticker, 'float_shares', None)
                fiscal_year = getattr(ticker, 'fiscal_year', None)
                state = getattr(ticker, 'state', None)
                p_s_ratio = getattr(ticker, 'p_s_ratio', None)
                nav = getattr(ticker, 'nav', None)
                
                if nav and isinstance(nav, str):
                    try:
                        nav = float(nav.replace(',', ''))
                    except ValueError:
                        nav = None

                # ğŸš¨ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ÙˆØ±ÙˆØ¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù†Ø§Ù‚Øµ
                if nav is None and base_volume == 1:
                    logger.debug(f"â© Ù†Ù…Ø§Ø¯ {symbol_name} Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ nav=None Ùˆ base_volume=1 Ø±Ø¯ Ø´Ø¯.")
                    continue

                # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ù…Ø§Ø¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ tse_index
                existing_symbol = db_session.query(ComprehensiveSymbolData).filter_by(
                    tse_index=tse_index
                ).first()
                
                if existing_symbol:
                    # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯
                    existing_symbol.symbol_name = symbol_name
                    existing_symbol.company_name = getattr(ticker, 'title', '')
                    existing_symbol.group_name = getattr(ticker, 'group_name', '')
                    existing_symbol.market_type = getattr(ticker, 'flow', '')
                    existing_symbol.base_volume = base_volume
                    existing_symbol.eps = eps
                    existing_symbol.p_e_ratio = p_e_ratio
                    existing_symbol.market_cap = market_cap
                    existing_symbol.float_shares = float_shares
                    existing_symbol.fiscal_year = fiscal_year
                    existing_symbol.state = state
                    existing_symbol.tse_index = tse_index
                    
                    if p_s_ratio is not None:
                        existing_symbol.p_s_ratio = p_s_ratio
                    if nav is not None:
                        existing_symbol.nav = nav
                    
                    existing_symbol.updated_at = now
                    updated_count += 1
                    
                else:
                    # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯
                    new_symbol = ComprehensiveSymbolData(
                        symbol_id=tse_index,
                        tse_index=tse_index,
                        symbol_name=symbol_name,
                        company_name=getattr(ticker, 'title', ''),
                        group_name=getattr(ticker, 'group_name', ''),
                        market_type=getattr(ticker, 'flow', ''),
                        base_volume=base_volume,
                        eps=eps,
                        p_e_ratio=p_e_ratio,
                        market_cap=market_cap,
                        float_shares=float_shares,
                        fiscal_year=fiscal_year,
                        state=state,
                        created_at=now,
                        updated_at=now
                    )
                    
                    if p_s_ratio is not None:
                        new_symbol.p_s_ratio = p_s_ratio
                    if nav is not None:
                        new_symbol.nav = nav
                    
                    db_session.add(new_symbol)
                    added_count += 1
                    
                # commit Ù‡Ø± 5 Ø±Ú©ÙˆØ±Ø¯ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² lock Ø·ÙˆÙ„Ø§Ù†ÛŒ
                if (added_count + updated_count) % 5 == 0:
                    db_session.commit()
                    time.sleep(3)
            
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù…Ø§Ø¯ {symbol_name}: {e}")
                db_session.rollback()
                continue

        db_session.commit()
        logger.info(f"âœ… Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯. {added_count} Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ØŒ {updated_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯.")
        return {"added": added_count, "updated": updated_count}
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø² pytse-client: {e}")
        db_session.rollback()
        return {"added": 0, "updated": 0}



def fetch_realtime_data_for_all_symbols(db_session: Session):
    """
    Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§
    """
    try:
        # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø±Ø¯
        # ÙØ¹Ù„Ø§Ù‹ return 0 Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø®Ø·Ø§ Ù†Ø¯Ù‡Ø¯
        logger.warning("âš ï¸ ØªØ§Ø¨Ø¹ fetch_realtime_data_for_all_symbols Ù‡Ù†ÙˆØ² Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª")
        return 0
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ: {e}")
        return 0


# ----------------------------
# ØªØ§Ø¨Ø¹ fetch_and_process_historical_data (Ù†Ø³Ø®Ù‡ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ Ùˆ Ø¬Ø§Ù…Ø¹)
# ----------------------------



def fetch_and_process_historical_data(
    db_session: Session,
    limit: Optional[int] = None,
    specific_symbols_list: Optional[List[str]] = None,
    limit_per_run: Optional[int] = None,
    days_limit: Optional[int] = None  # compatibility alias
) -> Tuple[int, str]:
    # normalize
    if limit is None and limit_per_run is not None:
        limit = limit_per_run
    # days_limit ÙØ¹Ù„Ø§Ù‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒØ´Ù‡ØŒ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø±ÙˆØ±
    """
    Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¬Ø§Ù…Ø¹ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø´Ø®Øµ ÛŒØ§ ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§.
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÛŒÙ‚ÛŒ/Ø­Ù‚ÙˆÙ‚ÛŒ Ø±Ø§ Ø¯Ø± ÛŒÚ© Ø­Ù„Ù‚Ù‡ ØªØ¬Ù…ÛŒØ¹ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    logger.info("ğŸ“ˆ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø§Ù…Ø¹...")
    updated_count = 0
    message = ""

    try:
        query = db_session.query(ComprehensiveSymbolData)
        if specific_symbols_list:
            symbol_conditions = []
            for symbol_identifier in specific_symbols_list:
                if str(symbol_identifier).isdigit():
                    symbol_conditions.append(ComprehensiveSymbolData.tse_index == str(symbol_identifier))
                else:
                    symbol_conditions.append(ComprehensiveSymbolData.symbol_name == symbol_identifier)
            query = query.filter(or_(*symbol_conditions))

        if limit:
            query = query.limit(limit)
        
        symbols_to_update = query.all()
        if not symbols_to_update:
            return 0, "No symbols to update."

        for sym in symbols_to_update:
            logger.info(f"ğŸ“Š Ø¢Ù¾Ø¯ÛŒØª Ø¯ÛŒØªØ§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¬Ø§Ù…Ø¹ Ø¨Ø±Ø§ÛŒ {sym.symbol_name} (ID: {sym.id})")
            
            last_db_date = db_session.query(
                func.max(HistoricalData.date)
            ).filter(HistoricalData.symbol_id == sym.id).scalar()
            
            try:
                if sym.tse_index:
                    ticker = tse.Ticker("", index=str(sym.tse_index))
                else:
                    ticker = tse.Ticker(sym.symbol_name)

                # 1. Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ (Ø´Ø§Ù…Ù„ OHLCV)
                df_history = ticker.history
                if df_history is None or df_history.empty:
                    logger.info(f"â„¹ï¸ Ø¯ÛŒØªØ§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {sym.symbol_name} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                    continue
                
                # 2. Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÛŒÙ‚ÛŒ/Ø­Ù‚ÙˆÙ‚ÛŒ
                df_client_types = ticker.client_types
                if df_client_types is None or df_client_types.empty:
                    # ===> Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ø®Ø§Ù„ÛŒ ÙÙ‚Ø· Ø¨Ø§ Ø³ØªÙˆÙ† date Ø¨Ø±Ø§ÛŒ merge Ù…ÙˆÙÙ‚
                    df_client_types = pd.DataFrame(columns=['date'])
                else:
                    # ØªØºÛŒÛŒØ± Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ Ù…Ø¯Ù„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                    df_client_types = df_client_types.rename(columns={
                        "individual_buy_count": "buy_count_i",
                        "corporate_buy_count": "buy_count_n",
                        "individual_sell_count": "sell_count_i",
                        "corporate_sell_count": "sell_count_n",
                        "individual_buy_vol": "buy_i_volume",
                        "corporate_buy_vol": "buy_n_volume",
                        "individual_sell_vol": "sell_i_volume",
                        "corporate_sell_vol": "sell_n_volume"
                    })                                             

                # 3. Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§ÛŒ ØªÙˆØ¶ÛŒØ­ÛŒ Ø´Ù…Ø§ Ø­Ø°Ù Ø´Ø¯ Ú†ÙˆÙ† Ù…Ù†Ø·Ù‚ Ø¯Ø± Ú©Ø¯ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
                df_history['date'] = pd.to_datetime(df_history['date'])
                if 'date' in df_client_types.columns:
                    df_client_types['date'] = pd.to_datetime(df_client_types['date'])
                
                df_merged = pd.merge(df_history, df_client_types, on='date', how='left')

                # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
                if last_db_date:
                    new_data = df_merged[df_merged['date'] > pd.to_datetime(last_db_date)].copy()
                else:
                    new_data = df_merged.copy()

                if new_data.empty:
                    logger.info(f"â„¹ï¸ Ø¯ÛŒØªØ§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {sym.symbol_name} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                    continue
                
                # ===> CHANGE START: Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
                # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª ØµØ­ÛŒØ­
                new_data.sort_values(by='date', inplace=True)
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª
                new_data['final'] = new_data['close']
                new_data['yesterday_price'] = new_data['close'].shift(1)
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ù‚ÛŒÙ…Øª (Price Changes)
                new_data['plc'] = new_data['close'] - new_data['yesterday_price']
                new_data['plp'] = (new_data['plc'] / new_data['yesterday_price']) * 100
                new_data['pcc'] = new_data['final'] - new_data['yesterday_price']
                new_data['pcp'] = (new_data['pcc'] / new_data['yesterday_price']) * 100
                
                # Ø¨Ù‡ØªØ±ÛŒÙ† ØªÙ‚Ø±ÛŒØ¨ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²Ø´ Ø¨Ø§Ø²Ø§Ø± ØªØ§Ø±ÛŒØ®ÛŒØŒ Ø§Ø±Ø²Ø´ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù‡Ù…Ø§Ù† Ø±ÙˆØ² Ø§Ø³Øª
                new_data['mv'] = new_data['value']

                # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª (inf) Ø¨Ø§ None ØªØ§ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø®Ø·Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ú©Ù†Ø¯
                new_data.replace([np.inf, -np.inf], None, inplace=True)
                # ===> CHANGE END

                # ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® Ù…ÛŒÙ„Ø§Ø¯ÛŒ Ø¨Ù‡ Ø´Ù…Ø³ÛŒ
                new_data['jdate'] = new_data['date'].apply(
                    lambda x: jdatetime.date.fromgregorian(date=x.date()).strftime("%Y-%m-%d")
                )
                
                # ØªØ¨Ø¯ÛŒÙ„ NaN Ø¨Ù‡ None Ùˆ Ø³Ù¾Ø³ Ø¨Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ
                records = new_data.where(pd.notnull(new_data), None).to_dict('records')
            
                historical_records = [
                    HistoricalData(
                        symbol_id=sym.symbol_id,
                        symbol_name=sym.symbol_name,
                        date=rec['date'].date(),
                        jdate=rec['jdate'],
                        open=rec.get('open'),
                        high=rec.get('high'),
                        low=rec.get('low'),
                        close=rec.get('close'),
                        volume=rec.get('volume'),
                        value=rec.get('value'),
                        num_trades=rec.get('count'),
                        
                        # ===> CHANGE START: Ø§ØµÙ„Ø§Ø­ Ú©Ù„ÛŒØ¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒØ´Ø¯Ù‡
                        final=rec.get('final'),
                        yesterday_price=rec.get('yesterday_price'),
                        plc=rec.get('plc'),
                        plp=rec.get('plp'),
                        pcc=rec.get('pcc'),
                        pcp=rec.get('pcp'),
                        mv=rec.get('mv'),
                        # ===> CHANGE END
                        
                        buy_count_i=rec.get('buy_count_i'),
                        buy_count_n=rec.get('buy_count_n'),
                        sell_count_i=rec.get('sell_count_i'),
                        sell_count_n=rec.get('sell_count_n'),
                        buy_i_volume=rec.get('buy_i_volume'),
                        buy_n_volume=rec.get('buy_n_volume'),
                        sell_i_volume=rec.get('sell_i_volume'),
                        sell_n_volume=rec.get('sell_n_volume'),
                        
                        # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù‚ Ø¨Ø§Ø²Ø§Ø± Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯ Ùˆ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ None Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù†Ø¯
                        zd1=None, qd1=None, pd1=None,
                        zo1=None, qo1=None, po1=None,
                        zd2=None, qd2=None, pd2=None,
                        zo2=None, qo2=None, po2=None,
                        zd3=None, qd3=None, pd3=None,
                        zo3=None, qo3=None, po3=None,
                        zd4=None, qd4=None, pd4=None,
                        zo4=None, qo4=None, po4=None,
                        zd5=None, qd5=None, pd5=None,
                        zo5=None, qo5=None, po5=None
                    ) for rec in records
                ]

                if historical_records:
                    db_session.bulk_save_objects(historical_records)
                    updated_count += len(historical_records)
                    logger.info(f"âœ… {len(historical_records)} Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ {sym.symbol_name} Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")
                    db_session.commit()
                
                safe_sleep(DEFAULT_PER_SYMBOL_DELAY)
            
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {sym.symbol_name}: {e}")
                logger.error(traceback.format_exc()) # Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† Ú©Ø§Ù…Ù„ Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ù‡ØªØ±
                db_session.rollback()
                continue
    
    except SQLAlchemyError as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡: {e}")
        db_session.rollback()
        return 0, f"Database error: {e}"
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ: {e}")
        return 0, f"An unexpected error occurred: {e}"
        
    message = f"âœ… Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯. {updated_count} Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯."
    logger.info(message)
    return updated_count, message



# ----------------------------
# ØªØ§Ø¨Ø¹ update_historical_data_limited (Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡)
# ----------------------------

#def update_historical_data_limited(DELETED)



# ----------------------------
# ØªØ§Ø¨Ø¹ Ø§Ø¬Ø±Ø§ÛŒ update_symbol_fundamental_data
# ----------------------------
def update_symbol_fundamental_data(
    db_session: Session,
    limit: Optional[int] = None,
    specific_symbols_list: Optional[List[str]] = None,
    limit_per_run: Optional[int] = None,
    days_limit: Optional[int] = None  # compatibility alias
) -> Tuple[int, str]:
    # normalize
    if limit is None and limit_per_run is not None:
        limit = limit_per_run
    # days_limit ÙØ¹Ù„Ø§Ù‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒØ´Ù‡ØŒ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø±ÙˆØ±
    """
    Ø¢Ù¾Ø¯ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù†ÛŒØ§Ø¯ÛŒ (Fundamental) Ù†Ù…Ø§Ø¯Ù‡Ø§.
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ EPS, P/E, Ùˆ Market Cap Ø±Ø§ Ø§Ø² Ticker Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    logger.info("ğŸ“ˆ Ø¢Ù¾Ø¯ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù†ÛŒØ§Ø¯ÛŒ...")
    updated_symbols_count = 0
    message = "Fundamental data updated successfully."

    try:
        query = db_session.query(ComprehensiveSymbolData)
        if specific_symbols_list:
            symbol_conditions = []
            for symbol_identifier in specific_symbols_list:
                if str(symbol_identifier).isdigit():
                    # ÙˆØ±ÙˆØ¯ÛŒ Ø¹Ø¯Ø¯ÛŒ â†’ tse_index
                    symbol_conditions.append(ComprehensiveSymbolData.tse_index == int(symbol_identifier))
                else:
                    # ÙˆØ±ÙˆØ¯ÛŒ Ù…ØªÙ†ÛŒ â†’ Ù†Ø§Ù… Ù†Ù…Ø§Ø¯
                    symbol_conditions.append(ComprehensiveSymbolData.symbol_name == symbol_identifier)
            
            query = query.filter(or_(*symbol_conditions))

        symbols_to_update = query.order_by(
            ComprehensiveSymbolData.last_fundamental_update_date.asc()
        ).limit(limit).all()

        if not symbols_to_update:
            return 0, "No symbols to update."

        for sym in symbols_to_update:
            logger.info(f"ğŸ“Š Ø¢Ù¾Ø¯ÛŒØª Ø¯ÛŒØªØ§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {sym.symbol_name} (TSEIndex: {sym.tse_index})")
            
            try:
                ticker = tse.Ticker(sym.symbol_name, index=sym.tse_index)

                
                # Ø¯Ø±ÛŒØ§ÙØª ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯ FundamentalData
                fundamental_data = db_session.query(FundamentalData).filter_by(symbol_id=sym.symbol_id).first()
                if not fundamental_data:
                    fundamental_data = FundamentalData(symbol_id=sym.symbol_id)
                    db_session.add(fundamental_data)
                
                # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ
                try:
                    fundamental_data.eps = ticker.eps
                # ===> Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ TypeError
                except (ValueError, TypeError):
                    fundamental_data.eps = None
                    logger.warning(f"âš ï¸ EPS Ø¨Ø±Ø§ÛŒ {sym.symbol_name} Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯.")
                
                try:
                    p_e = ticker.p_e_ratio
                    fundamental_data.p_e_ratio = p_e
                    # ===> Ø§ÙØ²ÙˆØ¯Ù†: Ù¾Ø± Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† pe Ø§Ú¯Ø± Ø¯Ø± Ù…Ø¯Ù„ Ø´Ù…Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
                    if hasattr(fundamental_data, 'pe'):
                        fundamental_data.pe = p_e
                # ===> Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ TypeError
                except (ValueError, TypeError):
                    fundamental_data.p_e_ratio = None
                    if hasattr(fundamental_data, 'pe'):
                        fundamental_data.pe = None
                    logger.warning(f"âš ï¸ P/E Ratio Ø¨Ø±Ø§ÛŒ {sym.symbol_name} Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯.")

                try:
                    fundamental_data.group_p_e_ratio = ticker.group_p_e_ratio
                # ===> Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ TypeError
                except (ValueError, TypeError):
                    fundamental_data.group_p_e_ratio = None

                try:
                    p_s = ticker.p_s_ratio
                    fundamental_data.p_s_ratio = p_s
                    # ===> Ø§ÙØ²ÙˆØ¯Ù†: Ù¾Ø± Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† psr Ø§Ú¯Ø± Ø¯Ø± Ù…Ø¯Ù„ Ø´Ù…Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
                    if hasattr(fundamental_data, 'psr'):
                        fundamental_data.psr = p_s
                # ===> Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ TypeError
                except (ValueError, TypeError):
                    fundamental_data.p_s_ratio = None
                    if hasattr(fundamental_data, 'psr'):
                        fundamental_data.psr = None
                
                # ===> Ø§ÙØ²ÙˆØ¯Ù†: Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø­Ø¬Ù… Ù…Ø¨Ù†Ø§ (Base Volume)
                try:
                    fundamental_data.base_volume = ticker.base_volume
                except (ValueError, TypeError):
                    fundamental_data.base_volume = None

                # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯Ù‡ Ø§Ø³Øª
                fundamental_data.total_shares = ticker.total_shares
                fundamental_data.float_shares = ticker.float_shares
                fundamental_data.market_cap = ticker.market_cap
                fundamental_data.fiscal_year = ticker.fiscal_year
                fundamental_data.last_update_date = date.today()
                
                # Ø¢Ù¾Ø¯ÛŒØª ØªØ§Ø±ÛŒØ® Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ
                sym.last_fundamental_update_date = date.today()
                
                updated_symbols_count += 1
                db_session.commit()
            
            except Exception as e:
                db_session.rollback()
                # ===> Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† Ú©Ø§Ù…Ù„ Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ù‡ØªØ±
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø¯ÛŒØªØ§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {sym.symbol_name}: {e}", exc_info=True)
                continue

    except Exception as e:
        db_session.rollback()
        # ===> Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† Ú©Ø§Ù…Ù„ Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ù‡ØªØ±
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø¯ÛŒØªØ§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {e}", exc_info=True)
        message = str(e)
        
    return updated_symbols_count, message




# ----------------------------
# ØªØ§Ø¨Ø¹ run_full_data_update (Ù†Ø³Ø®Ù‡ 10 Ø¬ÙˆÙ„Ø§ÛŒ)
# ----------------------------
# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ø¢Ø®Ø±ÛŒÙ† ØªØ§Ø±ÛŒØ® Ù‡Ø± Ù†Ù…Ø§Ø¯
def get_last_dates(db_session: Session) -> dict:
    """
    Ø¢Ø®Ø±ÛŒÙ† ØªØ§Ø±ÛŒØ® Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ HistoricalData Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…Ø§Ø¯ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    try:
        results = db_session.query(
            HistoricalData.symbol_id,
            func.max(HistoricalData.date)
        ).group_by(
            HistoricalData.symbol_id
        ).all()
        return {symbol_id: last_date for symbol_id, last_date in results}
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§: {e}")
        return {}


# Ø§ÛŒØ¬Ø§Ø¯ sessionmaker Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª connection pooling
def get_session_local():
    """Ø§ÛŒØ¬Ø§Ø¯ session local Ø¨Ø§ application context"""
    try:
        from flask import current_app
        with current_app.app_context():
            return sessionmaker(bind=db.engine)()
    except RuntimeError:
        # Ø§Ú¯Ø± Ø®Ø§Ø±Ø¬ Ø§Ø² application context Ù‡Ø³ØªÛŒÙ…
        return sessionmaker(bind=db.get_engine())()


def get_symbol_id(symbol_identifier: str) -> Optional[int]:
    if not symbol_identifier:
        return None

    session = db.session  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² session Ù¾ÛŒØ´â€ŒÙØ±Ø¶

    from models import ComprehensiveSymbolData 

    try:
        if str(symbol_identifier).isdigit():
            sym = session.query(ComprehensiveSymbolData.id).filter(
                ComprehensiveSymbolData.tse_index == str(symbol_identifier)
            ).first()
            if sym:
                return sym[0]
    except Exception:
        pass

    sym = session.query(ComprehensiveSymbolData.id).filter(
        ComprehensiveSymbolData.symbol_name == symbol_identifier
    ).first()

    if sym:
        logger.info(f"Symbol found. ID: {sym[0]}") # âœ… Ù„Ø§Ú¯ Ù…ÙˆÙÙ‚ÛŒØª
        return sym[0]
    
    logger.warning(f"Symbol NOT found in DB with name: {symbol_identifier}") # âœ… Ù„Ø§Ú¯ Ø¹Ø¯Ù… Ù…ÙˆÙÙ‚ÛŒØª
    return None




# ----------------------------
#Ø§ÛŒÙ† ÙØ§Ù†Ú©Ø´Ù† Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…Ø§Ø¯ ÛŒÚ© Ø±Ø¯ÛŒÙ real-time Ù…ÛŒâ€ŒØ³Ø§Ø²Ù‡ Ùˆ Ø¯Ø± HistoricalData ÛŒØ§ ÛŒÚ© Ø¬Ø¯ÙˆÙ„ Ø¬Ø¯Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
# ----------------------------
def fetch_realtime_snapshot(db_session: Session, symbol_name: str, symbol_id: int):
    try:
        ticker = tse.Ticker(symbol_name)

        # Ø¯ÛŒØªØ§ÛŒ Ø§ØµÙ„ÛŒ Ù‚ÛŒÙ…Øª
        final_price = ticker.last_price
        yesterday_price = ticker.yesterday_price
        adj_close = ticker.adj_close
        mv = ticker.market_cap
        eps = ticker.eps
        pe = ticker.p_e_ratio

        # ØªØºÛŒÛŒØ±Ø§Øª Ù‚ÛŒÙ…ØªÛŒ
        plc = final_price - yesterday_price if final_price and yesterday_price else None
        plp = (plc / yesterday_price * 100) if plc and yesterday_price else None
        pcc = adj_close - yesterday_price if adj_close and yesterday_price else None
        pcp = (pcc / yesterday_price * 100) if pcc and yesterday_price else None

        # Ø­Ù‚ÛŒÙ‚ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ
        ct = ticker.client_types
        buy_count_i = ct["individual_buy_count"].iloc[-1] if not ct.empty else None
        buy_count_n = ct["corporate_buy_count"].iloc[-1] if not ct.empty else None
        sell_count_i = ct["individual_sell_count"].iloc[-1] if not ct.empty else None
        sell_count_n = ct["corporate_sell_count"].iloc[-1] if not ct.empty else None
        buy_i_volume = ct["individual_buy_vol"].iloc[-1] if not ct.empty else None
        buy_n_volume = ct["corporate_buy_vol"].iloc[-1] if not ct.empty else None
        sell_i_volume = ct["individual_sell_vol"].iloc[-1] if not ct.empty else None
        sell_n_volume = ct["corporate_sell_vol"].iloc[-1] if not ct.empty else None

        # Ø¹Ù…Ù‚ Ø¨Ø§Ø²Ø§Ø± (Ø³Ø·Ø­ Ûµ)
        orderbook = ticker.get_ticker_real_time_info_response()
        buy_orders = orderbook.buy_orders
        sell_orders = orderbook.sell_orders

        dom_fields = {}
        for i in range(5):
            dom_fields[f"zd{i+1}"] = buy_orders[i].count if len(buy_orders) > i else None
            dom_fields[f"qd{i+1}"] = buy_orders[i].volume if len(buy_orders) > i else None
            dom_fields[f"pd{i+1}"] = buy_orders[i].price if len(buy_orders) > i else None
            dom_fields[f"zo{i+1}"] = sell_orders[i].count if len(sell_orders) > i else None
            dom_fields[f"qo{i+1}"] = sell_orders[i].volume if len(sell_orders) > i else None
            dom_fields[f"po{i+1}"] = sell_orders[i].price if len(sell_orders) > i else None

        # Ø³Ø§Ø®Øª Ø±Ú©ÙˆØ±Ø¯ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        snapshot = HistoricalData(
            symbol_id=symbol_id,
            symbol_name=symbol_name,
            date=date.today(),
            final=final_price,
            yesterday_price=yesterday_price,
            plc=plc,
            plp=plp,
            pcc=pcc,
            pcp=pcp,
            #mv=mv,
            #eps=eps,
            #pe=pe,
            buy_count_i=buy_count_i,
            buy_count_n=buy_count_n,
            sell_count_i=sell_count_i,
            sell_count_n=sell_count_n,
            buy_i_volume=buy_i_volume,
            buy_n_volume=buy_n_volume,
            sell_i_volume=sell_i_volume,
            sell_n_volume=sell_n_volume,
            **dom_fields
        )

        db_session.add(snapshot)


        fundamental = db_session.query(FundamentalData).filter_by(symbol_id=symbol_id).first()
        if fundamental:
            #fundamental.eps = eps
            fundamental.pe = pe
            #fundamental.market_cap = mv
        else:
            fundamental = FundamentalData(
                symbol_id=symbol_id,
                #eps=eps,
                pe=pe
                #market_cap=mv
            )
            db_session.add(fundamental)




        db_session.commit()
        return True, f"âœ… Real-time snapshot stored for {symbol_name}"

    except Exception as e:
        db_session.rollback()
        return False, f"âŒ Error fetching snapshot for {symbol_name}: {e}"




# ----------------------------
# ØªØ§Ø¨Ø¹ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„
# ----------------------------
def run_full_data_update(
    db_session: Session = None,
    limit_per_run: int = 100,
    specific_symbols_list: list = None,

    update_fundamental: bool = True,
    update_realtime: bool = True,
    update_technical: bool = True
):
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„ Ø¯ÛŒØªØ§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ (ØªØ§Ø±ÛŒØ®ÛŒ + ØªÚ©Ù†ÛŒÚ©Ø§Ù„ + Ø¨Ù†ÛŒØ§Ø¯ÛŒ + Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ)
    Ø§Ú¯Ø± db_session Ù¾Ø§Ø³ Ø¯Ø§Ø¯Ù‡ Ù†Ø´ÙˆØ¯ØŒ Ø®ÙˆØ¯Ø´ session Ø±Ø§ Ø§Ø² extensions.db Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
    """
    if db_session is None:
        db_session = db.session

    logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù†Ù…Ø§Ø¯Ù‡Ø§...")

    results = {
        "historical": {"count": 0, "message": ""},
        "technical": {"count": 0, "message": ""},
        "fundamental": {"count": 0, "message": ""},
        "realtime": {"count": 0, "message": ""},
        "candlestick": {"count": 0, "message": ""}
    }

    # 1. Ø§Ø¨ØªØ¯Ø§ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø±Ø§ Ø§Ø² pytse-client Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†ÛŒÙ…
    try:
        logger.info("ğŸ“¥ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø² pytse-client...")
        symbol_update_result = fetch_symbols_from_pytse_client(db_session, limit=None)
        logger.info(f"âœ… Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯: {symbol_update_result}")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§: {e}")

    # 2. Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ
    try:
        logger.info("ğŸ“Š Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ...")
        processed_hist_count, hist_msg = fetch_and_process_historical_data(
            db_session,
            limit=limit_per_run,
            specific_symbols_list=specific_symbols_list,

        )
        results["historical"]["count"] = processed_hist_count
        results["historical"]["message"] = hist_msg
        logger.info(hist_msg)
    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª ØªØ§Ø±ÛŒØ®ÛŒ: {e}"
        results["historical"]["message"] = error_msg
        logger.error(error_msg)

    # 3. Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ
    if update_realtime:
        try:
            logger.info("â° Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ...")
            realtime_count = fetch_realtime_data_for_all_symbols(db_session)
            results["realtime"]["count"] = realtime_count
            results["realtime"]["message"] = f"âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ {realtime_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯"
            logger.info(results["realtime"]["message"])

            # âœ… Ú¯Ø±ÙØªÙ† snapshot Ú©Ø§Ù…Ù„ Ø§Ø² Ù‡Ù…Ù‡ Ù†Ù…Ø§Ø¯Ù‡Ø§
            all_symbols = db_session.query(ComprehensiveSymbolData).all()
            snapshot_count = 0
            for sym in all_symbols:
                success, msg = fetch_realtime_snapshot(db_session, sym.symbol_name, sym.id)
                logger.info(msg)
                if success:
                    snapshot_count += 1

            results["realtime"]["message"] += f" | âœ… snapshot Ø¨Ø±Ø§ÛŒ {snapshot_count} Ù†Ù…Ø§Ø¯ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯"

        except Exception as e:
            error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ: {e}"
            results["realtime"]["message"] = error_msg
            logger.error(error_msg)



    # 4. Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ
    if update_fundamental:
        try:
            logger.info("ğŸ“ˆ Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ...")
            
            # ğŸ› ï¸ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ø®Ø±ÛŒÙ† Ø¢Ù¾Ø¯ÛŒØª
            query = db_session.query(ComprehensiveSymbolData)
            if specific_symbols_list:
                symbol_conditions = [or_(ComprehensiveSymbolData.symbol_name == s, ComprehensiveSymbolData.tse_index == s) for s in specific_symbols_list]
                query = query.filter(or_(*symbol_conditions))
            
            # âš ï¸ Ø§ØµÙ„Ø§Ø­ ÙÛŒÙ„ØªØ± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ØªÙˆÙ† Ø¯Ø±Ø³Øª Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² timedelta
            symbols_to_update = query.filter(
                (ComprehensiveSymbolData.last_fundamental_update_date.is_(None)) | 
                (ComprehensiveSymbolData.last_fundamental_update_date < (date.today() - timedelta(days=7)))
            ).order_by(ComprehensiveSymbolData.last_fundamental_update_date.asc()).limit(limit_per_run).all()
            
            fundamental_count = 0
            for symbol in symbols_to_update:
                try:
                    updated_count, msg = update_symbol_fundamental_data(db_session, specific_symbols_list=[symbol.tse_index])
                    if updated_count > 0:
                        fundamental_count += updated_count
                        # âš ï¸ Ø¢Ù¾Ø¯ÛŒØª Ø³ØªÙˆÙ† Ø²Ù…Ø§Ù† Ø¢Ø®Ø±ÛŒÙ† Ø¢Ù¾Ø¯ÛŒØª
                        symbol.last_fundamental_update_date = date.today()
                        db_session.add(symbol)
                        db_session.commit()
                    time.sleep(0.1)
                except Exception as e:
                    logger.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ {symbol.symbol_name}: {e}")
                    db_session.rollback()
                    continue
            
            results["fundamental"]["count"] = fundamental_count
            results["fundamental"]["message"] = f"âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {fundamental_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯"
            logger.info(results["fundamental"]["message"])
            
        except Exception as e:
            error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {e}"
            results["fundamental"]["message"] = error_msg
            logger.error(error_msg)
            db_session.rollback()

    # 5. Ø¢Ù¾Ø¯ÛŒØª ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ùˆ 6. ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ
    if update_technical:
        try:
            logger.info("ğŸ“‰ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„...")
            technical_count, tech_msg = run_technical_analysis(
                db_session,
                limit=limit_per_run,
                symbols_list=specific_symbols_list
            )
            results["technical"]["count"] = technical_count
            results["technical"]["message"] = tech_msg
            logger.info(results["technical"]["message"])
            
            
            # =======================================================
            # ğŸ•¯ï¸ Ú¯Ø§Ù… 6: ØªØ´Ø®ÛŒØµ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ (Ø¬Ø¯ÛŒØ¯)
            # Ø§ÛŒÙ† Ú¯Ø§Ù… Ø¨Ù„Ø§ÙØ§ØµÙ„Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ùˆ Ø¯Ø§Ø®Ù„ Ù‡Ù…Ø§Ù† Ø¨Ù„ÙˆÚ© try Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
            # =======================================================
            logger.info("ğŸ•¯ï¸ Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ...")
            try:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ù…Ø§Ù† Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§
                candlestick_count = run_candlestick_detection(
                    db_session, 
                    limit=limit_per_run,
                    symbols_list=specific_symbols_list
                )
                # âš ï¸ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†ØªØ§ÛŒØ¬
                results["candlestick"] = {
                    "count": candlestick_count,
                    "message": f"âœ… ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ {candlestick_count} Ù†Ù…Ø§Ø¯ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
                }
                logger.info(results["candlestick"]["message"])
            except Exception as e:
                error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {e}"
                results["candlestick"] = {"count": 0, "message": error_msg}
                logger.error(error_msg)
            

        except Exception as e:
            # Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© catchØŒ Ø®Ø·Ø§ÛŒ Ø§ØµÙ„ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
            error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {e}"
            results["technical"]["message"] = error_msg
            logger.error(error_msg)
            db_session.rollback()

    logger.info("âœ… Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø§ØªÙ…Ø§Ù… Ø±Ø³ÛŒØ¯.")
    
    # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
    summary = f"""
ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬ Ø¢Ù¾Ø¯ÛŒØª:
â€¢ ØªØ§Ø±ÛŒØ®ÛŒ: {results['historical']['count']} Ù†Ù…Ø§Ø¯ - {results['historical']['message']}
â€¢ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ: {results['realtime']['count']} Ù†Ù…Ø§Ø¯ - {results['realtime']['message']}
â€¢ Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {results['fundamental']['count']} Ù†Ù…Ø§Ø¯ - {results['fundamental']['message']}
â€¢ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {results['technical']['count']} Ù†Ù…Ø§Ø¯ - {results['technical']['message']}
â€¢ Ø´Ù…Ø¹ÛŒ: {results['candlestick']['count']} Ù†Ù…Ø§Ø¯ - {results['candlestick']['message']}
    """
    logger.info(summary)

    # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ callerÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒØŒ Ø¯Ùˆ Ù…Ù‚Ø¯Ø§Ø± return Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    return results




# ----------------------------
# ØªØ§Ø¨Ø¹ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡
# ----------------------------
def run_daily_update(
    db_session: Session,
    limit: int = 200, # limit now acts as BATCH_SIZE
    update_fundamental: bool = True,
    specific_symbols_list: Optional[List[str]] = None,
):
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ (batch) Ø¨Ø±Ø§ÛŒ ØªØ¶Ù…ÛŒÙ† Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§.
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ØªØ§ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ù†Ù…Ø§Ø¯ Ø¢Ù¾Ø¯ÛŒØªâ€ŒÙ†Ø´Ø¯Ù‡â€ŒØ§ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø¯Ø± ÛŒÚ© Ø­Ù„Ù‚Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    """
    # âš ï¸ Ø­ØªÙ…Ø§Ù‹ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ importsÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² (date, timedelta, logger, or_) Ø¯Ø± Ø¨Ø§Ù„Ø§ÛŒ ÙØ§ÛŒÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ù†Ø¯.
    logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ...")

    results = {
        "historical": {"total_count": 0},
        "technical": {"total_count": 0},
        "candlestick": {"total_count": 0, "message": ""},
        "fundamental": {"count": 0, "message": ""}
    }
    
    run_count = 0
    
    # ===============================
    # Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
    # ===============================
    while True:
        run_count += 1
        logger.info(f"--- Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡ Ø´Ù…Ø§Ø±Ù‡ {run_count} ---")

        today = date.today()
        
        # Ú¯Ø§Ù… Û±: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÛŒÚ© Ø¯Ø³ØªÙ‡ Ø§Ø² Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ù…Ø±ÙˆØ² Ø¢Ù¾Ø¯ÛŒØª Ù†Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
        symbols_to_update_query = db_session.query(ComprehensiveSymbolData).filter(
            or_(
                ComprehensiveSymbolData.last_historical_update_date.is_(None),
                ComprehensiveSymbolData.last_historical_update_date < today
            )
        )
        
        # Ø§Ú¯Ø± Ù„ÛŒØ³Øª Ø®Ø§ØµÛŒ Ø§Ø² Ù†Ù…Ø§Ø¯Ù‡Ø§ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯
        if specific_symbols_list:
            symbol_conditions = [
                or_(
                    ComprehensiveSymbolData.symbol_name == s,
                    ComprehensiveSymbolData.tse_index == s
                )
                for s in specific_symbols_list
            ]
            symbols_to_update_query = symbols_to_update_query.filter(or_(*symbol_conditions))


        # Ø§Ø¹Ù…Ø§Ù„ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø³ØªÙ‡
        symbols_in_batch = symbols_to_update_query.limit(limit).all()

        if not symbols_in_batch:
            logger.info("âœ… ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ù…Ø±ÙˆØ² Ø¨Ù‡â€ŒØ±ÙˆØ² Ù‡Ø³ØªÙ†Ø¯. Ù¾Ø§ÛŒØ§Ù† Ø¹Ù…Ù„ÛŒØ§Øª Historical/Technical.")
            break

        symbol_ids_to_process = [s.tse_index for s in symbols_in_batch]
        logger.info(f"ğŸ“Š ÛŒØ§ÙØª Ø´Ø¯ {len(symbol_ids_to_process)} Ù†Ù…Ø§Ø¯ Ø¯Ø± Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª.")

        # Ú¯Ø§Ù… Û²: Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡
        hist_count, hist_msg = fetch_and_process_historical_data(
            db_session,
            specific_symbols_list=symbol_ids_to_process
        )
        results["historical"]["total_count"] += hist_count
        logger.info(hist_msg)

        # Ú¯Ø§Ù… Û³ Ùˆ Û´ ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
        if hist_count > 0:
            # Ú¯Ø§Ù… Û³: Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
            tech_count, tech_msg = run_technical_analysis(
                db_session,
                symbols_list=symbol_ids_to_process
            )
            results["technical"]["total_count"] += tech_count
            logger.info(tech_msg)

            # Ú¯Ø§Ù… Û´: ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ
            try:
                candlestick_count = run_candlestick_detection(
                    db_session, 
                    symbols_list=symbol_ids_to_process
                )
                results["candlestick"]["total_count"] += candlestick_count
                logger.info(f"âœ… ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ {candlestick_count} Ù†Ù…Ø§Ø¯ Ø¯Ø± Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡: {e}")
        else:
            logger.warning("Ø¯Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ú¯Ø§Ù…â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Skip Ø´Ø¯.")
        

        # ----------------------------------------------------
        # ğŸ’¥ğŸ’¥ğŸ’¥ FIX Ø­Ù„Ù‚Ù‡ Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ØªØ§Ø±ÛŒØ® Ø¢Ù¾Ø¯ÛŒØª ğŸ’¥ğŸ’¥ğŸ’¥
        # Ø§ÛŒÙ† Ú©Ø§Ø± ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø§ÛŒÙ† Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¯Ø± Ø¯ÙˆØ± Ø¨Ø¹Ø¯ÛŒ Ú©ÙˆØ¦Ø±ÛŒ ÙÛŒÙ„ØªØ± Ø´ÙˆÙ†Ø¯.
        # Ø§ÛŒÙ† Ø¢Ù¾Ø¯ÛŒØª Ø¨Ø§ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ØŒ Ø­ØªÛŒ Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (hist_count=0).
        # ----------------------------------------------------
        for symbol in symbols_in_batch:
            symbol.last_historical_update_date = today # ØªÙ†Ø¸ÛŒÙ… Ø¨Ù‡ Ø§Ù…Ø±ÙˆØ²
            db_session.add(symbol)
        
        # IMPROVEMENT: Commit changes after each successful batch
        try:
            db_session.commit()
            logger.info(f"âœ… ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø³ØªÙ‡ {run_count} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø«Ø¨Øª Ø´Ø¯.")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø«Ø¨Øª ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø³ØªÙ‡ {run_count}: {e}")
            db_session.rollback()
            break
            
    # ===============================
    # Ú¯Ø§Ù… Ûµ: Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ (Ø®Ø§Ø±Ø¬ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ)
    # ===============================
    if update_fundamental:
        try:
            logger.info("ğŸ“ˆ Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ...")

            # limit_per_run Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù‡Ù…Ø§Ù† limit ÙˆØ±ÙˆØ¯ÛŒ ØªØ§Ø¨Ø¹ Ø§Ø³Øª
            limit_per_run = limit

            query = db_session.query(ComprehensiveSymbolData)
            
            # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù„ÛŒØ³Øª Ø®Ø§ØµÛŒ Ø§Ø² Ù†Ù…Ø§Ø¯Ù‡Ø§
            if specific_symbols_list:
                symbol_conditions = [
                    or_(
                        ComprehensiveSymbolData.symbol_name == s,
                        ComprehensiveSymbolData.tse_index == s
                    )
                    for s in specific_symbols_list
                ]
                query = query.filter(or_(*symbol_conditions))

            # Ú©ÙˆØ¦Ø±ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø¯Ø§Ø±Ù†Ø¯
            symbols_to_update_fund = query.filter(
                (ComprehensiveSymbolData.last_fundamental_update_date.is_(None)) |
                (ComprehensiveSymbolData.last_fundamental_update_date < (date.today() - timedelta(days=3)))
            ).order_by(
                ComprehensiveSymbolData.last_fundamental_update_date.asc()
            ).limit(limit_per_run).all()
            
            fundamental_count = 0
            for symbol in symbols_to_update_fund:
                try:
                    # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ (Ø¨Ø± Ø§Ø³Ø§Ø³ tse_index)
                    updated_count, msg = update_symbol_fundamental_data(
                        db_session,
                        specific_symbols_list=[symbol.tse_index]
                    )
                    
                    if updated_count > 0:
                        fundamental_count += updated_count
                        # Ø¢Ù¾Ø¯ÛŒØª ØªØ§Ø±ÛŒØ® Ø¢Ø®Ø±ÛŒÙ† Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ
                        symbol.last_fundamental_update_date = date.today()
                        db_session.add(symbol)
                        db_session.commit()
                    
                    # ğŸ’¡ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÛŒÚ© ØªØ£Ø®ÛŒØ± Ú©ÙˆØªØ§Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ÙØ´Ø§Ø± Ø¨Ø± API
                    time.sleep(0.1) 
                    
                except Exception as e:
                    db_session.rollback()
                    logger.warning(
                        f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù†Ù…Ø§Ø¯ {symbol.symbol_name} ({symbol.tse_index}): {e}"
                    )
                    continue

            results["fundamental"]["count"] = fundamental_count
            results["fundamental"]["message"] = f"âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {fundamental_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯"
            logger.info(results["fundamental"]["message"])

        except Exception as e:
            error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {e}"
            results["fundamental"]["message"] = error_msg
            logger.error(error_msg, exc_info=True)
            db_session.rollback()
            
    else:
        results["fundamental"]["message"] = "Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Skip Ø´Ø¯."
        logger.info(results["fundamental"]["message"])

    # ===============================
    # Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø¨Ø§Ø²Ú¯Ø´Øª Ù†ØªØ§ÛŒØ¬
    # ===============================
    final_message = (
        f"ğŸ Ø¢Ù¾Ø¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. "
        f"Historical: {results['historical']['total_count']}, "
        f"Technical: {results['technical']['total_count']}, "
        f"Candlestick: {results['candlestick']['total_count']}, "
        f"Fundamental: {results['fundamental']['count']}."
    )
    logger.info(final_message)
    return results





# ----------------------------
# ØªØ§Ø¨Ø¹ initial_populate_all_symbols_and_data (Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡)
# ----------------------------

def initial_populate_all_symbols_and_data(db_session, limit: int = None):
    """
    ØªØ§Ø¨Ø¹ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø± Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒØŒ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ
    """
    try:
        logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§ÙˆÙ„ÛŒÙ‡ Ù¾Ø± Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø¨ÛŒØ³...")
        
        # 1. Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø² pytse-client Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ComprehensiveSymbolData
        logger.info("ğŸ“¥ Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø² pytse-client...")
        symbol_result = fetch_symbols_from_pytse_client(db_session, limit)
        
        added_count = symbol_result.get("added", 0)
        updated_count = symbol_result.get("updated", 0)
        
        if added_count > 0 or updated_count > 0:
            logger.info(f"ğŸ“Š {added_count} Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ØŒ {updated_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯.")
            
            # 2. Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ
            logger.info("ğŸ“ˆ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ...")
            processed_count, msg = fetch_and_process_historical_data(
                db_session, 
                limit=limit
            )
            logger.info(msg)
            
            # 3. Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ
            logger.info("â° Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ...")
            realtime_count = fetch_realtime_data_for_all_symbols(db_session)
            logger.info(f"âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ {realtime_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯")
            
            # 4. Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ
            logger.info("ğŸ“Š Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ...")

            # âš ï¸ Ú©ÙˆØ¦Ø±ÛŒ Ú¯Ø±ÙØªÙ† Ù‡Ù…Ø²Ù…Ø§Ù† id Ùˆ tse_index (Ø¨Ø§ Ø§Ø¹Ù…Ø§Ù„ limit Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
            symbols_to_update = db_session.query(
                ComprehensiveSymbolData.id,
                ComprehensiveSymbolData.tse_index
            ).limit(limit).all()

            fundamental_count = 0
            for symbol_id, tse_index in symbols_to_update:
                try:
                    # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ (Ø¨Ø± Ø§Ø³Ø§Ø³ tse_index)
                    updated_count, msg = update_symbol_fundamental_data(
                        db_session,
                        specific_symbols_list=[tse_index]
                    )
                    fundamental_count += updated_count

                    # ğŸ’¡ Ù†Ú©ØªÙ‡: 'time.sleep(0.2)' Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª Ø­Ø°Ù Ø´Ø¯.

                except Exception as e:
                    # ğŸ› ï¸ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ ØªØ±Ø§Ú©Ù†Ø´ Rollback Ø´ÙˆØ¯ ØªØ§ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù‚ÙÙ„ Ù†Ø´ÙˆØ¯
                    db_session.rollback()
                    logger.warning(
                        f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù†Ù…Ø§Ø¯ {tse_index} (ID={symbol_id}): {e}"
                    )
                    continue  # Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ Ø¨Ø¹Ø¯ÛŒ

            logger.info(f"âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {fundamental_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯")

            # 5. Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
            logger.info("ğŸ“‰ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„...")
            try:
                # âš ï¸ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ ÙÙ‚Ø· Ø¨Ù‡ id Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ…
                symbol_ids = [sid for sid, _ in symbols_to_update]

                run_technical_analysis(
                    db_session,
                    limit=limit,
                    symbols_list=symbol_ids
                )
                logger.info("âœ… ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¬Ø±Ø§ Ø´Ø¯")

            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {e}")
            
            total_processed = added_count + updated_count
            success_msg = f"""
âœ… ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§ÙˆÙ„ÛŒÙ‡ Ù¾Ø± Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ú©Ø§Ù…Ù„ Ø´Ø¯:
â€¢ {added_count} Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
â€¢ {updated_count} Ù†Ù…Ø§Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯
â€¢ {processed_count} Ù†Ù…Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù†Ø¯
â€¢ {realtime_count} Ù†Ù…Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù†Ø¯
â€¢ {fundamental_count} Ù†Ù…Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù†Ø¯
            """
            logger.info(success_msg)
            
            return {
                "added": added_count,
                "updated": updated_count,
                "historical": processed_count,
                "realtime": realtime_count,
                "fundamental": fundamental_count,
                "message": success_msg
            }
        else:
            logger.info("â„¹ï¸ Ù‡ÛŒÚ† Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ÛŒ Ø§Ø¶Ø§ÙÙ‡ ÛŒØ§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ø´Ø¯.")
            return {
                "added": 0,
                "updated": 0,
                "historical": 0,
                "realtime": 0,
                "fundamental": 0,
                "message": "â„¹ï¸ Ù‡ÛŒÚ† Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ÛŒ Ø§Ø¶Ø§ÙÙ‡ ÛŒØ§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ø´Ø¯."
            }
            
    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§ÙˆÙ„ÛŒÙ‡ Ù¾Ø± Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}"
        logger.error(error_msg)
        import traceback
        logger.error(traceback.format_exc())
        
        return {
            "added": 0,
            "updated": 0,
            "historical": 0,
            "realtime": 0,
            "fundamental": 0,
            "message": error_msg
        }


# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
# ----------------------------

# ----------------------------
# ØªØ§Ø¨Ø¹ run_technical_analysis (Ù†Ø³Ø®Ù‡ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ)
# ----------------------------

def run_technical_analysis(db_session: Session, limit: int = None, symbols_list: list = None, batch_size: int = 200):
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¯Ø± Ø¨Ú†â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…ØµØ±Ù Ø²ÛŒØ§Ø¯ Ø­Ø§ÙØ¸Ù‡.
    """
    try:
        logger.info("ğŸ“ˆ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„...")

        # Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª ÛŒÚ©ØªØ§ Ø§Ø² Ù†Ù…Ø§Ø¯Ù‡Ø§
        symbol_ids_query = db_session.query(HistoricalData.symbol_id).distinct()
        if symbols_list:
            symbol_ids_query = symbol_ids_query.filter(HistoricalData.symbol_id.in_(symbols_list))

        all_symbols = [row[0] for row in symbol_ids_query.all()]
        total_symbols = len(all_symbols)
        logger.info(f"ğŸ” Ù…Ø¬Ù…ÙˆØ¹ {total_symbols} Ù†Ù…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ ÛŒØ§ÙØª Ø´Ø¯")

        processed_count = 0
        success_count = 0
        error_count = 0

        # Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø± Ø¨Ú†â€ŒÙ‡Ø§ÛŒ 200ØªØ§ÛŒÛŒ
        for i in range(0, total_symbols, batch_size):
            batch_symbols = all_symbols[i:i + batch_size]
            logger.info(f"ğŸ“¦ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ú† {i // batch_size + 1}: Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ {i + 1} ØªØ§ {min(i + batch_size, total_symbols)}")

            query = db_session.query(
                HistoricalData.symbol_id, HistoricalData.symbol_name, HistoricalData.date, HistoricalData.jdate, 
                HistoricalData.open, HistoricalData.close, HistoricalData.high, HistoricalData.low, 
                HistoricalData.volume, HistoricalData.final, HistoricalData.yesterday_price, HistoricalData.plc, 
                HistoricalData.plp, HistoricalData.pcc, HistoricalData.pcp, HistoricalData.mv, 
                HistoricalData.buy_count_i, HistoricalData.buy_count_n, HistoricalData.sell_count_i, 
                HistoricalData.sell_count_n, HistoricalData.buy_i_volume, HistoricalData.buy_n_volume, 
                HistoricalData.sell_i_volume, HistoricalData.sell_n_volume
            ).filter(HistoricalData.symbol_id.in_(batch_symbols))

            query = query.order_by(HistoricalData.symbol_id, HistoricalData.date)
            historical_data = query.all()

            if not historical_data:
                logger.warning(f"âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ú† ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                continue

            columns = [
                'symbol_id', 'symbol_name', 'date', 'jdate', 'open', 'close', 'high', 'low', 'volume',
                'final', 'yesterday_price', 'plc', 'plp', 'pcc', 'pcp', 'mv',
                'buy_count_i', 'buy_count_n', 'sell_count_i', 'sell_count_n',
                'buy_i_volume', 'buy_n_volume', 'sell_i_volume', 'sell_n_volume'
            ]
            df = pd.DataFrame(historical_data, columns=columns)

            grouped = df.groupby('symbol_id')

            for symbol_id, group_df in grouped:
                if limit is not None and processed_count >= limit:
                    break

                processed_count += 1
                try:
                    df_indicators = calculate_all_indicators(group_df)
                    save_technical_indicators(db_session, symbol_id, df_indicators)
                    success_count += 1

                    if processed_count % 10 == 0:
                        logger.info(f"ğŸ“Š Ù¾ÛŒØ´Ø±ÙØª ØªØ­Ù„ÛŒÙ„: {processed_count}/{total_symbols} Ù†Ù…Ø§Ø¯")

                except Exception as e:
                    error_count += 1
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù†Ù…Ø§Ø¯ {symbol_id}: {e}")
                    db_session.rollback()

            # ğŸ”¹ Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡â€ŒÛŒ DataFrame Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± Ø¨Ú†
            del df
            import gc
            gc.collect()

        logger.info(f"âœ… ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ù…ÙˆÙÙ‚: {success_count} | Ø®Ø·Ø§: {error_count}")
        return success_count, f"ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯. {success_count} Ù…ÙˆÙÙ‚ØŒ {error_count} Ø®Ø·Ø§"

    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {e}"
        logger.error(error_msg)
        db_session.rollback()
        return 0, error_msg



# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ (Ø¨Ø§ ÙØ±Ø¶ import Ø¨ÙˆØ¯Ù† pd, np, Tuple, logger)
# ----------------------------

# ØªÙˆØ§Ø¨Ø¹ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±ØŒ Ú†ÙˆÙ† Ø§Ø² Series Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯):
def calculate_sma(series: pd.Series, period: int) -> pd.Series:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø³Ø§Ø¯Ù‡ (SMA)"""
    return series.rolling(window=period).mean()

def calculate_volume_ma(series: pd.Series, period: int) -> pd.Series:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø­Ø¬Ù…"""
    return series.rolling(window=period).mean()

def calculate_rsi(series: pd.Series, period: int = 14) -> pd.Series:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ RSI"""
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    with np.errstate(divide='ignore', invalid='ignore'):
        rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_macd(series: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ MACD"""
    ema_fast = series.ewm(span=fast, adjust=False).mean()
    ema_slow = series.ewm(span=slow, adjust=False).mean()
    macd = ema_fast - ema_slow
    macd_signal = macd.ewm(span=signal, adjust=False).mean()
    macd_histogram = macd - macd_signal
    return macd, macd_signal, macd_histogram

def calculate_bollinger_bands(series: pd.Series, period: int = 20, std_dev: int = 2) -> Tuple[pd.Series, pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Bollinger Bands"""
    middle = series.rolling(window=period).mean()
    std = series.rolling(window=period).std()
    upper = middle + (std * std_dev)
    lower = middle - (std * std_dev)
    return upper, middle, lower

def calculate_atr(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR (Average True Range)"""
    tr1 = high - low
    tr2 = abs(high - close.shift())
    tr3 = abs(low - close.shift())
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = tr.rolling(window=period).mean()
    return atr

def calculate_stochastic(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14, smooth_k: int = 3, smooth_d: int = 3) -> Tuple[pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Stochastic Oscillator (%K Ùˆ %D)."""
    high = high.squeeze()
    low = low.squeeze()
    close = close.squeeze()
    
    # ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù†ÛŒØ² Ø§Ø² Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯ ÛŒØ§ Ø³Ø±ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§Ø´Ù†Ø¯
    df_stoch = pd.DataFrame({'high': high, 'low': low, 'close': close}).apply(pd.to_numeric, errors='coerce')
    if df_stoch.isnull().all().all() or len(df_stoch.dropna()) < window:
        nan_series = pd.Series([np.nan] * len(close), index=close.index)
        return nan_series, nan_series

    low_min = df_stoch['low'].rolling(window=window).min()
    high_max = df_stoch['high'].rolling(window=window).max()
    
    k = 100 * ((df_stoch['close'] - low_min) / (high_max - low_min).replace(0, np.nan))
    k = k.fillna(50) 
    
    d = k.rolling(window=smooth_k).mean()
    
    return k.astype(float).reindex(close.index), d.astype(float).reindex(close.index)

# --- ØªÙˆØ§Ø¨Ø¹ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ ---

def calculate_squeeze_momentum(df: pd.DataFrame, bb_window=20, bb_std=2, kc_window=20, kc_mult=1.5) -> Tuple[pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Squeeze Momentum Indicator."""
    # âœ… Ø§ØµÙ„Ø§Ø­: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
    close = pd.to_numeric(df['close'].squeeze(), errors='coerce')
    high = pd.to_numeric(df['high'].squeeze(), errors='coerce')
    low = pd.to_numeric(df['low'].squeeze(), errors='coerce')
    
    # Bollinger Bands
    bb_ma = close.rolling(window=bb_window).mean()
    bb_std_dev = close.rolling(window=bb_window).std()
    bb_upper = bb_ma + (bb_std_dev * bb_std)
    bb_lower = bb_ma - (bb_std_dev * bb_std)

    # Keltner Channels
    tr = pd.concat([(high - low), abs(high - close.shift()), abs(low - close.shift())], axis=1).max(axis=1)
    atr = tr.rolling(window=kc_window).mean()
    kc_ma = close.rolling(window=kc_window).mean()
    kc_upper = kc_ma + (atr * kc_mult)
    kc_lower = kc_ma - (atr * kc_mult)
    
    # Squeeze condition (Ø®Ø±ÙˆØ¬ÛŒ Boolean)
    # ğŸ’¡ Ø§ØµÙ„Ø§Ø­ Ø¹Ø¨Ø§Ø±Øª Ø§ØµÙ„ÛŒ: ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Squeeze Ø²Ù…Ø§Ù†ÛŒ Ø±ÙˆØ´Ù† Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Bollinger Bands Ø¯Ø±ÙˆÙ† Keltner Channels Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ø¯.
    squeeze_on = (bb_lower > kc_lower) & (bb_upper < kc_upper)

    # Momentum
    highest_high = high.rolling(window=bb_window).max()
    lowest_low = low.rolling(window=bb_window).min()
    avg_hl = (highest_high + lowest_low) / 2
    avg_close = close.rolling(window=bb_window).mean()
    momentum = (close - (avg_hl + avg_close) / 2)
    
    momentum_smoothed = momentum.rolling(window=bb_window).apply(lambda x: np.polyfit(np.arange(len(x)), x, 1)[0], raw=True)

    # ØªØ¨Ø¯ÛŒÙ„ ØµØ±ÛŒØ­ Boolean Ø¨Ù‡ Integer Ùˆ reindex Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ ØªØ±Ø§Ø²
    return squeeze_on.astype(int).reindex(df.index), momentum_smoothed.reindex(df.index)

def calculate_halftrend(df: pd.DataFrame, amplitude=2, channel_deviation=2) -> Tuple[pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± HalfTrend."""
    try:
        # âœ… Ø§ØµÙ„Ø§Ø­: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        high = pd.to_numeric(df['high'].squeeze(), errors='coerce')
        low = pd.to_numeric(df['low'].squeeze(), errors='coerce')
        close = pd.to_numeric(df['close'].squeeze(), errors='coerce')

        # Ù…Ù†Ø·Ù‚ calculate_atr
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² 100 Ø¨Ù‡ Ø¬Ø§ÛŒ Ù¾Ø±ÛŒÙˆØ¯ Ù¾ÛŒØ´ ÙØ±Ø¶ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ø¯ Ø§ØµÙ„ÛŒ Ø´Ù…Ø§)
        atr = tr.rolling(window=100).mean() / 2 

        high_price = high.rolling(window=amplitude).max()
        low_price = low.rolling(window=amplitude).min()
        
        # Ù…Ù†Ø·Ù‚ calculate_sma
        highma = high_price.rolling(window=amplitude).mean()
        lowma = low_price.rolling(window=amplitude).mean()

        # Ø¢Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª
        trend_list = [0] * len(df)
        next_trend_list = [0] * len(df)
        
        # FIX: Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ Ø¹Ø¯Ø¯ÛŒ (Ø¨Ø¯ÙˆÙ† Series)
        close_list = close.to_list()
        lowma_list = lowma.to_list()
        highma_list = highma.to_list()
        
        for i in range(1, len(df)):
            # Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± NaN Ø¯Ø± MAÙ‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚ÛŒÙ…Øª Ø¨Ø³ØªÙ‡â€ŒØ´Ø¯Ù† Ø±ÙˆØ² Ù‚Ø¨Ù„ (fallback)
            prev_lowma = lowma_list[i-1] if i > 0 and not pd.isna(lowma_list[i-1]) else close_list[i-1] if i > 0 else close_list[i]
            prev_highma = highma_list[i-1] if i > 0 and not pd.isna(highma_list[i-1]) else close_list[i-1] if i > 0 else close_list[i]
            
            if next_trend_list[i-1] == 1:
                if close_list[i] < prev_lowma:
                    trend_list[i] = -1
                else:
                    trend_list[i] = 1
            else:
                if close_list[i] > prev_highma:
                    trend_list[i] = 1
                else:
                    trend_list[i] = -1

            if trend_list[i] == trend_list[i-1]:
                next_trend_list[i] = trend_list[i-1]
            else:
                next_trend_list[i] = trend_list[i]

        df['trend'] = pd.Series(trend_list, index=df.index, dtype=int)
        
        halftrend_signal = df['trend'].reindex(df.index)
        
        # Ø®Ø±ÙˆØ¬ÛŒ (Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø±ÙˆÙ†Ø¯ Ú©Ø§Ù…Ù„)
        return halftrend_signal, halftrend_signal 

    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ HalfTrend Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯: {e}", exc_info=True)
        nan_series = pd.Series([np.nan] * len(df), index=df.index)
        return nan_series, nan_series

def calculate_support_resistance_break(df: pd.DataFrame, window=50) -> Tuple[pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø¯Ù‡ Ø´Ú©Ø³Øª Ù…Ù‚Ø§ÙˆÙ…Øª."""
    # âœ… Ø§ØµÙ„Ø§Ø­: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
    close = pd.to_numeric(df['close'].squeeze(), errors='coerce')
    high = pd.to_numeric(df['high'].squeeze(), errors='coerce')
    
    resistance = high.shift(1).rolling(window=window).max()
    
    # Ø´Ú©Ø³Øª Ø²Ù…Ø§Ù†ÛŒ Ø±Ø® Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ù‚ÛŒÙ…Øª Ù¾Ø§ÛŒØ§Ù†ÛŒ Ø§Ù…Ø±ÙˆØ² Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ù…Ù‚Ø§ÙˆÙ…Øª Ø¯ÛŒØ±ÙˆØ² Ø¨Ø§Ø´Ø¯ (Ø®Ø±ÙˆØ¬ÛŒ Boolean)
    resistance_broken = close > resistance
    
    # ØªØ¨Ø¯ÛŒÙ„ ØµØ±ÛŒØ­ Boolean Ø¨Ù‡ Integer Ùˆ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ float Ø¨Ø±Ø§ÛŒ resistance
    resistance_broken_int = resistance_broken.astype(int)
    
    return resistance.astype(float).reindex(df.index), resistance_broken_int.reindex(df.index)


# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
# ----------------------------
def calculate_all_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ…Ø§Ù… Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ DataFrame.
    """
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø®Ø§Ù„ÛŒ Ù†ÛŒØ³Øª Ùˆ Ø¯Ø§Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª
    if df.empty or not {'open', 'high', 'low', 'close', 'volume'}.issubset(df.columns):
        logger.warning("DataFrame Ø®Ø§Ù„ÛŒ Ø§Ø³Øª ÛŒØ§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯.")
        return df

    try: # <--- Ø´Ø±ÙˆØ¹ Ø¨Ù„ÙˆÚ© try Ø§ØµÙ„ÛŒ
        # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù†ÙˆØ¹ Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø±
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if not pd.api.types.is_numeric_dtype(df[col]):
                df[col] = pd.to_numeric(df[col], errors='coerce')
        df.dropna(subset=['open', 'high', 'low', 'close', 'volume'], inplace=True)

        if df.empty:
            logger.warning("Ù¾Ø³ Ø§Ø² ØªØ¨Ø¯ÛŒÙ„ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒØŒ Ø¯ÛŒØªØ§ÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø¨Ø§Ù‚ÛŒ Ù†Ù…Ø§Ù†Ø¯.")
            return df
        
        # --- Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ ---
        df['RSI'] = calculate_rsi(df['close'])
        
        macd, signal, histogram = calculate_macd(df['close'])
        df['MACD'] = macd
        df['MACD_Signal'] = signal
        df['MACD_Histogram'] = histogram
        
        df['SMA_20'] = calculate_sma(df['close'], 20)
        df['SMA_50'] = calculate_sma(df['close'], 50)
        
        upper, middle, lower = calculate_bollinger_bands(df['close'])
        df['Bollinger_Upper'] = upper
        df['Bollinger_Middle'] = middle
        df['Bollinger_Lower'] = lower
        
        df['Volume_MA_20'] = calculate_volume_ma(df['volume'], 20)
        df['ATR'] = calculate_atr(df['high'], df['low'], df['close'])

        # --- Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ---
        # 1. Ù…Ø­Ø§Ø³Ø¨Ù‡ Stochastic
        stochastic_k, stochastic_d = calculate_stochastic(df['high'], df['low'], df['close'])
        df['Stochastic_K'] = stochastic_k
        df['Stochastic_D'] = stochastic_d
        
        # 3. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
        try: # <--- Ø¨Ù„ÙˆÚ© try Ø¯Ø§Ø®Ù„ÛŒ
            # ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø§Ú©Ù†ÙˆÙ† Ø§Ø² Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ 'close', 'high', 'low' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.
            squeeze_on, _ = calculate_squeeze_momentum(df)
            df['squeeze_on'] = squeeze_on
            
            halftrend_signal, _ = calculate_halftrend(df)
            df['halftrend_signal'] = halftrend_signal
            
            resistance_level, resistance_broken = calculate_support_resistance_break(df)
            df['resistance_level_50d'] = resistance_level
            df['resistance_broken'] = resistance_broken
        
        except Exception as e: # <--- Ø¨Ù„ÙˆÚ© except Ø¯Ø§Ø®Ù„ÛŒ
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø¯Ø± ØªØ§Ø¨Ø¹ calculate_all_indicators (Ø¯Ø§Ø®Ù„ÛŒ): {e}", exc_info=True)
            # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø§Ø± Ø¨Ø§ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŒ Ù¾Ø³ df Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ….
            pass # Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§Ø¨Ø¹ Ø¨Ù‡ Ù…Ø³ÛŒØ± Ø§ØµÙ„ÛŒ Ø®ÙˆØ¯ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ù‡Ø¯ Ùˆ df Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯.

    except Exception as e: # <--- Ø¨Ù„ÙˆÚ© except Ø§ØµÙ„ÛŒ Ú©Ù‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯!
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÛŒØ§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯: {e}", exc_info=True)
        return df
        
    return df # <--- Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ


# ----------------------------
# ØªØ§Ø¨Ø¹ CandlestickPatternDetection
# ----------------------------
def run_candlestick_detection(db_session: Session, limit: int = None, symbols_list: list = None):
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬.
    Ø§Ø² Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø­Ø°Ù Ùˆ Ø¯Ø±Ø¬ (Delete & Insert) Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    (Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ MemoryError Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù…Ø§Ø¯ Ø¨Ù‡ Ù†Ù…Ø§Ø¯)
    """
    from datetime import datetime
    import pandas as pd
    
    # Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Logger ÙØ±Ø¶ Ø¨Ø± import Ø¨ÙˆØ¯Ù† Ù‡Ø³ØªÙ†Ø¯

    try:
        logger.info("ğŸ•¯ï¸ Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ...")
        
        # 1. Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª symbol_id Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„ (Ø§Ø² ComprehensiveSymbolData ÛŒØ§ HistoricalData)
        # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ÙÚ† Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ø§Ø¨ØªØ¯Ø§ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆÙ†Ø¯ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
        
        # âš ï¸ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª symbol_idÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± HistoricalData
        base_query = db_session.query(HistoricalData.symbol_id).distinct()
        
        if symbols_list:
            base_query = base_query.filter(HistoricalData.symbol_id.in_(symbols_list)) 
            
        symbol_ids_to_process = [str(s[0]) for s in base_query.all()]
        
        if not symbol_ids_to_process:
            logger.warning("âš ï¸ Ù‡ÛŒÚ† Ù†Ù…Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return 0
            
        logger.info(f"ğŸ” ÛŒØ§ÙØª Ø´Ø¯ {len(symbol_ids_to_process)} Ù†Ù…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ")

        success_count = 0
        records_to_insert = []
        
        # 2. Ø­Ù„Ù‚Ù‡ Ø²Ø¯Ù† Ø±ÙˆÛŒ Ù‡Ø± Ù†Ù…Ø§Ø¯ Ùˆ ÙÚ† Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
        processed_count = 0
        for symbol_id in symbol_ids_to_process:
            if limit is not None and processed_count >= limit:
                break
            
            try:
                # ğŸ’¡ Ù†Ù‚Ø·Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡: ÙÚ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯
                # Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø­Ø¯Ø§Ú©Ø«Ø± Ø¨Ù‡ Ú†Ù†Ø¯ Ø±ÙˆØ² Ø§Ø®ÛŒØ± Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ… (Ù…Ø«Ù„Ø§Ù‹ 10 Ø±ÙˆØ² Ø¢Ø®Ø±).
                # Ø§ÛŒÙ† Ú©Ø§Ø± Ø§Ø² Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¯Ø± RAM Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
                
                historical_data_query = db_session.query(HistoricalData).filter(
                    HistoricalData.symbol_id == symbol_id
                ).order_by(HistoricalData.date.desc()).limit(30) # ğŸ‘ˆ ÙÚ† Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ Û³Û° Ø±ÙˆØ² Ø§Ø®ÛŒØ±
                
                # ğŸ’¥ Ø§Ú©Ù†ÙˆÙ† Ø§ÛŒÙ† Ú©ÙˆØ¦Ø±ÛŒ ÙÙ‚Ø· Ø­Ø¯ÙˆØ¯ Û³Û° Ø±Ú©ÙˆØ±Ø¯ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ØŒ Ù†Ù‡ Ú©Ù„ ØªØ§Ø±ÛŒØ®Ú†Ù‡.
                historical_data = historical_data_query.all() 
                
                if len(historical_data) < 5: 
                    continue 

                # ğŸ’¡ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame
                df = pd.DataFrame([row.__dict__ for row in historical_data])
                if '_sa_instance_state' in df.columns:
                    df = df.drop(columns=['_sa_instance_state']) 
                
                # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ ioc[-1] Ø±ÙˆØ² Ø¬Ø¯ÛŒØ¯ Ø§Ø³Øª (Ø¨Ø± Ø§Ø³Ø§Ø³ date ØµØ¹ÙˆØ¯ÛŒ)
                df.sort_values(by='date', inplace=True) 

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…:
                today_record_dict = df.iloc[-1].to_dict()
                yesterday_record_dict = df.iloc[-2].to_dict()
                
                # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯Ùˆ:
                patterns = check_candlestick_patterns(
                    today_record_dict, 
                    yesterday_record_dict, 
                    df # Ú©Ù„ DataFrame Ù…Ø­Ø¯ÙˆØ¯ Ø´Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Û³Û° Ø±ÙˆØ²Ù‡)
                )
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡
                if patterns:
                    now = datetime.now()
                    current_jdate = today_record_dict['jdate']
                    for pattern in patterns:
                        records_to_insert.append({
                            'symbol_id': symbol_id,
                            'jdate': current_jdate,
                            'pattern_name': pattern,
                            'created_at': now, 
                            'updated_at': now
                        })
                    success_count += 1
                
                processed_count += 1
                if processed_count % 50 == 0:
                    logger.info(f"ğŸ•¯ï¸ Ù¾ÛŒØ´Ø±ÙØª ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {processed_count}/{len(symbol_ids_to_process)} Ù†Ù…Ø§Ø¯")

            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_id}: {e}", exc_info=True)
                # Ø¨Ø¯ÙˆÙ† rollback Ø¯Ø± Ø§ÛŒÙ† Ø³Ø·Ø­ (Ø²ÛŒØ±Ø§ bulk insert Ø¨Ø¹Ø¯Ø§Ù‹ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯)
                
        logger.info(f"âœ… ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ {success_count} Ù†Ù…Ø§Ø¯ Ø¨Ø§ Ø§Ù„Ú¯Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
                
        # 3. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Delete & Insert - Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
        if records_to_insert:
            # Ø§Ù„Ù) Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ® Ùˆ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
            last_jdate = records_to_insert[0]['jdate'] 
            processed_symbol_ids = list({record['symbol_id'] for record in records_to_insert})
            
            # Ø¨) Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
            try:
                db_session.query(CandlestickPatternDetection).filter(
                    CandlestickPatternDetection.symbol_id.in_(processed_symbol_ids),
                    CandlestickPatternDetection.jdate == last_jdate
                ).delete(synchronize_session=False) 
                
                db_session.commit()
                logger.info(f"ğŸ—‘ï¸ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ù‚Ø¨Ù„ÛŒ ({len(processed_symbol_ids)} Ù†Ù…Ø§Ø¯) Ø¨Ø±Ø§ÛŒ {last_jdate} Ø­Ø°Ù Ø´Ø¯Ù†Ø¯.")
                
            except Exception as e:
                db_session.rollback()
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {e}", exc_info=True)
                return success_count
                
            # Ø¬) Ø¯Ø±Ø¬ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            db_session.bulk_insert_mappings(CandlestickPatternDetection, records_to_insert)
            db_session.commit()
            logger.info(f"âœ… {len(records_to_insert)} Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø±Ø¬ Ø´Ø¯.")
        else:
            logger.info("â„¹ï¸ Ù‡ÛŒÚ† Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù…Ø¹ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

        return success_count

    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {e}", exc_info=True)
        db_session.rollback()
        return 0



# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ (Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ)
# ----------------------------

def update_comprehensive_symbol_data(db_session: Session, symbols_list: list = None):
    """
    Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§
    """
    try:
        logger.info("ğŸ“Š Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ...")
        
        query = db_session.query(ComprehensiveSymbolData)
        if symbols_list:
            # Ø§Ú¯Ø± Ù„ÛŒØ³Øª Ø´Ø§Ù…Ù„ idÙ‡Ø§ÛŒ ComprehensiveSymbolData Ø§Ø³Øª
            if all(isinstance(x, int) for x in symbols_list):
                query = query.filter(ComprehensiveSymbolData.id.in_(symbols_list))
            # Ø§Ú¯Ø± Ù„ÛŒØ³Øª Ø´Ø§Ù…Ù„ Ù†Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø³Øª
            else:
                query = query.filter(ComprehensiveSymbolData.symbol_name.in_(symbols_list))
            
        symbols = query.all()
        
        processed_count = 0
        for symbol in symbols:
            try:
                # Ø§ØµÙ„Ø§Ø­ÛŒÙ‡: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² symbol_name Ùˆ symbol_index
                fundamental_data = fetch_fundamental_data(symbol.symbol_name, symbol.tse_index) # Ø¯Ø± Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ø§ØµÙ„Ø§Ø­ Ú©Ù†ÛŒØ¯.
                
                if fundamental_data:
                    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ
                    save_fundamental_data(db_session, symbol.id, fundamental_data)
                    processed_count += 1
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol.symbol_name}: {e}")
        
        logger.info(f"âœ… Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯. {processed_count} Ù†Ù…Ø§Ø¯ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯.")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {e}")

def fetch_fundamental_data(symbol_name: str, symbol_index: str) -> dict:
    """
    Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø§Ø² pytse-client
    """
    try:
        # Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø² symbol_name Ùˆ symbol_index Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
        ticker = tse.Ticker(symbol_name, index=symbol_index)
        
        fundamental_data = {
            'p_e': ticker.p_e_ratio,
            'eps': ticker.eps,
            'p_s': ticker.p_s_ratio,
            'p_b': ticker.p_b_ratio,
            'dividend_yield': ticker.dividend_yield,
            'market_cap': ticker.market_cap,
            'shares_outstanding': ticker.shares_outstanding,
            'float_shares': ticker.float_shares,
            'base_volume': ticker.base_volume,
            'sector_pe': ticker.sector_pe,
            'group_pe': ticker.group_pe,
            'sector_pb': ticker.sector_pb,
            'group_pb': ticker.group_pb,
            'sector_eps': ticker.sector_eps,
            'group_eps': ticker.group_eps,
            'sector_dividend_yield': ticker.sector_dividend_yield,
            'group_dividend_yield': ticker.group_dividend_yield
        }
        
        return fundamental_data
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {symbol_name}: {e}")
        return {}

def save_fundamental_data(db_session: Session, symbol_id: int, fundamental_data: dict):
    """
    Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    """
    try:
        existing_data = db_session.query(FundamentalData).filter_by(symbol_id=symbol_id).first()
        
        if existing_data:
            # Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ¬ÙˆØ¯
            for key, value in fundamental_data.items():
                if hasattr(existing_data, key):
                    setattr(existing_data, key, value)
        else:
            # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯
            new_data = FundamentalData(
                symbol_id=symbol_id,
                p_e=fundamental_data.get('p_e'),
                eps=fundamental_data.get('eps'),
                p_s=fundamental_data.get('p_s'),
                p_b=fundamental_data.get('p_b'),
                dividend_yield=fundamental_data.get('dividend_yield'),
                market_cap=fundamental_data.get('market_cap'),
                shares_outstanding=fundamental_data.get('shares_outstanding'),
                float_shares=fundamental_data.get('float_shares'),
                base_volume=fundamental_data.get('base_volume'),
                sector_pe=fundamental_data.get('sector_pe'),
                group_pe=fundamental_data.get('group_pe'),
                sector_pb=fundamental_data.get('sector_pb'),
                group_pb=fundamental_data.get('group_pb'),
                sector_eps=fundamental_data.get('sector_eps'),
                group_eps=fundamental_data.get('group_eps'),
                sector_dividend_yield=fundamental_data.get('sector_dividend_yield'),
                group_dividend_yield=fundamental_data.get('group_dividend_yield')
            )
            db_session.add(new_data)
            
        db_session.commit()
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {e}")
        db_session.rollback()


def save_technical_indicators(db_session: Session, symbol_id: int, df: pd.DataFrame):
    """
    Ø°Ø®ÛŒØ±Ù‡ (Ø¯Ø±Ø¬ ÛŒØ§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ) Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ TechnicalIndicatorData.
    """
    # ØªØ¨Ø¯ÛŒÙ„ symbol_id Ø¨Ù‡ Ø±Ø´ØªÙ‡
    symbol_id_str = str(symbol_id)

    # ØªØ¶Ù…ÛŒÙ† ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ† symbol_id
    if 'symbol_id' not in df.columns:
        df['symbol_id'] = symbol_id_str

    # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ ÙÙ‚Ø· Ø¯Ø§Ø®Ù„ DataFrame (Ø¶Ø±ÙˆØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØµØ­Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² .copy() Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² SettingWithCopyWarning
    df_unique = df.drop_duplicates(subset=['symbol_id', 'jdate'], keep='last').copy()
    df_to_save = df_unique.dropna(subset=['RSI', 'MACD'])

    if df_to_save.empty:
        logger.debug(f"âš ï¸ Ù¾Ø³ Ø§Ø² Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒØŒ Ù‡ÛŒÚ† Ø³Ø·Ø± Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_id_str} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª.")
        return
    
    updates_count = 0
    inserts_count = 0
    
    try: # Ø¨Ù„ÙˆÚ© try/except Ø¨Ø§ÛŒØ¯ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø¨Ø§Ø´Ø¯ ØªØ§ commit ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯
        for _, row in df_to_save.iterrows():
            # 1. Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø±Ú©ÙˆØ±Ø¯ Ù…ÙˆØ¬ÙˆØ¯
            existing = db_session.query(TechnicalIndicatorData).filter_by(
                symbol_id=symbol_id_str,
                jdate=row['jdate']
            ).first()

            # 2. Ù…Ù†Ø·Ù‚ Ø¯Ø±Ø¬ ÛŒØ§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ (Upsert)
            if existing:
                # âœ… Update Ø±Ú©ÙˆØ±Ø¯ Ù…ÙˆØ¬ÙˆØ¯
                existing.close_price = row.get('close')
                existing.RSI = row.get('RSI')
                existing.MACD = row.get('MACD')
                existing.MACD_Signal = row.get('MACD_Signal')
                existing.MACD_Hist = row.get('MACD_Histogram')
                existing.SMA_20 = row.get('SMA_20')
                existing.SMA_50 = row.get('SMA_50')
                existing.Bollinger_High = row.get('Bollinger_Upper')
                existing.Bollinger_Low = row.get('Bollinger_Lower')
                existing.Bollinger_MA = row.get('Bollinger_Middle')
                existing.Volume_MA_20 = row.get('Volume_MA_20')
                existing.ATR = row.get('ATR')
                existing.Stochastic_K = row.get('Stochastic_K')
                existing.Stochastic_D = row.get('Stochastic_D')
                existing.squeeze_on = bool(row.get('squeeze_on'))
                existing.halftrend_signal = row.get('halftrend_signal')
                existing.resistance_level_50d = row.get('resistance_level_50d')
                existing.resistance_broken = bool(row.get('resistance_broken'))
                
                # ØªÙˆØ¬Ù‡: Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø´Ù…Ø§ `updated_at` Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡â€ŒØ±ÙˆØ² Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ø¢Ù† Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ² Ú©Ù†ÛŒØ¯
                # existing.updated_at = datetime.now() 
                
                updates_count += 1
            else:
                # âœ… Insert Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯
                indicator = TechnicalIndicatorData(
                    symbol_id=symbol_id_str,
                    jdate=row['jdate'],
                    close_price=row.get('close'),
                    RSI=row.get('RSI'),
                    MACD=row.get('MACD'),
                    MACD_Signal=row.get('MACD_Signal'),
                    MACD_Hist=row.get('MACD_Histogram'),
                    SMA_20=row.get('SMA_20'),
                    SMA_50=row.get('SMA_50'),
                    Bollinger_High=row.get('Bollinger_Upper'),
                    Bollinger_Low=row.get('Bollinger_Lower'),
                    Bollinger_MA=row.get('Bollinger_Middle'),
                    Volume_MA_20=row.get('Volume_MA_20'),
                    ATR=row.get('ATR'),
                    Stochastic_K=row.get('Stochastic_K'),
                    Stochastic_D=row.get('Stochastic_D'),
                    squeeze_on=bool(row.get('squeeze_on')),
                    halftrend_signal=row.get('halftrend_signal'),
                    resistance_level_50d=row.get('resistance_level_50d'),
                    resistance_broken=bool(row.get('resistance_broken'))
                )
                db_session.add(indicator)
                inserts_count += 1

        # Commit Ú©Ø±Ø¯Ù† ØªØ±Ø§Ú©Ù†Ø´ Ù¾Ø³ Ø§Ø² Ø§ØªÙ…Ø§Ù… Ø­Ù„Ù‚Ù‡
        db_session.commit()
        logger.info(f"âœ… Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_id_str} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡/Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯Ù†Ø¯. (Ø¯Ø±Ø¬: {inserts_count}ØŒ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {updates_count})")
        
    except Exception as e:
        db_session.rollback()
        # ğŸ’¡ Ø§ÛŒÙ† Ø®Ø·Ø§ Ø§Ú©Ù†ÙˆÙ† ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ Ø±Ø® Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø¯Ùˆ Ø±Ø´ØªÙ‡ Ù‡Ù…Ø²Ù…Ø§Ù† (concurrent processes) Ø¨Ø®ÙˆØ§Ù‡Ù†Ø¯ ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯ Ø±Ø§ Ø¯Ø±Ø¬ Ú©Ù†Ù†Ø¯
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_id_str}: {e}")



# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
# ----------------------------

def cleanup_memory():
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡"""
    try:
        gc.collect()
        current_memory = check_memory_usage_mb()
        if current_memory > MEMORY_LIMIT_MB:
            logger.warning(f"âš ï¸ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ Ø¨Ø§Ù„Ø§: {current_memory:.2f} MB")
    except Exception as e:
        logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡: {e}")

def batch_process_symbols(symbols: list, process_func: callable, batch_size: int = DEFAULT_BATCH_SIZE):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§"""
    results = []
    for i in range(0, len(symbols), batch_size):
        batch = symbols[i:i + batch_size]
        batch_results = []
        
        for symbol in batch:
            try:
                result = process_func(symbol)
                batch_results.append(result)
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù…Ø§Ø¯ {symbol}: {e}")
                batch_results.append(None)
                
        results.extend(batch_results)
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± Ø¨Ú†
        cleanup_memory()
        
    return results

# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ Ùˆ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯
# ----------------------------

def get_data_status_report(db_session: Session) -> dict:
    """
    Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    """
    try:
        total_symbols = db_session.query(ComprehensiveSymbolData).count()
        symbols_with_historical = db_session.query(
            func.count(distinct(HistoricalData.symbol_id))
        ).scalar()
        symbols_with_technical = db_session.query(
            func.count(distinct(TechnicalIndicatorData.symbol_id))
        ).scalar()
        symbols_with_fundamental = db_session.query(
            func.count(distinct(FundamentalData.symbol_id))
        ).scalar()
        
        total_historical_records = db_session.query(HistoricalData).count()
        total_technical_records = db_session.query(TechnicalIndicatorData).count()
        
        latest_historical_date = db_session.query(
            func.max(HistoricalData.date)
        ).scalar()
        
        return {
            'total_symbols': total_symbols,
            'symbols_with_historical': symbols_with_historical,
            'symbols_with_technical': symbols_with_technical,
            'symbols_with_fundamental': symbols_with_fundamental,
            'total_historical_records': total_historical_records,
            'total_technical_records': total_technical_records,
            'latest_historical_date': latest_historical_date,
            'historical_coverage': f"{symbols_with_historical}/{total_symbols}",
            'technical_coverage': f"{symbols_with_technical}/{total_symbols}",
            'fundamental_coverage': f"{symbols_with_fundamental}/{total_symbols}"
        }
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙ‡ÛŒÙ‡ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª: {e}")
        return {}

def check_data_consistency(db_session: Session) -> dict:
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    """
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ Ø¯Ø§Ø±Ù†Ø¯ Ø§Ù…Ø§ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù†Ø¯Ø§Ø±Ù†Ø¯
        symbols_missing_technical = db_session.query(ComprehensiveSymbolData).filter(
            ComprehensiveSymbolData.id.in_(
                db_session.query(HistoricalData.symbol_id).distinct()
            ),
            ~ComprehensiveSymbolData.id.in_(
                db_session.query(TechnicalIndicatorData.symbol_id).distinct()
            )
        ).count()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ Ø¯Ø§Ø±Ù†Ø¯ Ø§Ù…Ø§ Ø¯Ø§Ø¯Ù‡ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯
        symbols_missing_fundamental = db_session.query(ComprehensiveSymbolData).filter(
            ComprehensiveSymbolData.id.in_(
                db_session.query(HistoricalData.symbol_id).distinct()
            ),
            ~ComprehensiveSymbolData.id.in_(
                db_session.query(FundamentalData.symbol_id).distinct()
            )
        ).count()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        duplicate_historical = db_session.query(
            HistoricalData.symbol_id,
            HistoricalData.date,
            func.count('*')
        ).group_by(
            HistoricalData.symbol_id,
            HistoricalData.date
        ).having(func.count('*') > 1).count()
        
        return {
            'symbols_missing_technical': symbols_missing_technical,
            'symbols_missing_fundamental': symbols_missing_fundamental,
            'duplicate_historical_records': duplicate_historical,
            'issues_found': symbols_missing_technical + symbols_missing_fundamental + duplicate_historical > 0
        }
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}")
        return {}

# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ùˆ ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
# ----------------------------

def repair_missing_data(db_session: Session, data_type: str = 'all', limit: int = 50):
    """
    ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ø±ÙØªÙ‡
    """
    try:
        logger.info(f"ğŸ”§ Ø´Ø±ÙˆØ¹ ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ø±ÙØªÙ‡ ({data_type})...")
        
        if data_type in ['historical', 'all']:
            # ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ
            symbols_missing_historical = db_session.query(ComprehensiveSymbolData).filter(
                ~ComprehensiveSymbolData.id.in_(
                    db_session.query(HistoricalData.symbol_id).distinct()
                )
            ).limit(limit).all()
            
            if symbols_missing_historical:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² idÙ‡Ø§ÛŒ ComprehensiveSymbolData
                symbol_ids = [s.id for s in symbols_missing_historical]
                processed_count, msg = fetch_and_process_historical_data(
                    db_session,
                    limit_per_run=limit,
                    specific_symbols_list=symbol_ids
                )
                logger.info(f"âœ… ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ: {msg}")
        
        if data_type in ['technical', 'all']:
            # ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
            symbols_missing_technical = db_session.query(ComprehensiveSymbolData).filter(
                ComprehensiveSymbolData.id.in_(
                    db_session.query(HistoricalData.symbol_id).distinct()
                ),
                ~ComprehensiveSymbolData.id.in_(
                    db_session.query(TechnicalIndicatorData.symbol_id).distinct()
                )
            ).limit(limit).all()
            
            if symbols_missing_technical:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² idÙ‡Ø§ÛŒ ComprehensiveSymbolData
                symbol_ids = [s.id for s in symbols_missing_technical]
                run_technical_analysis(
                    db_session,
                    limit=limit,
                    symbols_list=symbol_ids
                )
                logger.info(f"âœ… ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø±Ø§ÛŒ {len(symbols_missing_technical)} Ù†Ù…Ø§Ø¯")
        
        if data_type in ['fundamental', 'all']:
            # ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ
            symbols_missing_fundamental = db_session.query(ComprehensiveSymbolData).filter(
                ComprehensiveSymbolData.id.in_(
                    db_session.query(HistoricalData.symbol_id).distinct()
                ),
                ~ComprehensiveSymbolData.id.in_(
                    db_session.query(FundamentalData.symbol_id).distinct()
                )
            ).limit(limit).all()
            
            if symbols_missing_fundamental:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² idÙ‡Ø§ÛŒ ComprehensiveSymbolData
                symbol_ids = [s.id for s in symbols_missing_fundamental]
                update_comprehensive_symbol_data(
                    db_session,
                    symbols_list=symbol_ids
                )
                logger.info(f"âœ… ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {len(symbols_missing_fundamental)} Ù†Ù…Ø§Ø¯")
                
        logger.info("âœ… ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯.")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}")
        raise

def cleanup_duplicate_data(db_session: Session):
    """
    Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
    """
    try:
        logger.info("ğŸ§¹ Ø´Ø±ÙˆØ¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ...")
        
        # Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        duplicate_historical = db_session.query(
            HistoricalData.symbol_id,
            HistoricalData.date
        ).group_by(
            HistoricalData.symbol_id,
            HistoricalData.date
        ).having(func.count('*') > 1).all()
        
        for symbol_id, date in duplicate_historical:
            # Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† ØªÙ†Ù‡Ø§ Ø§ÙˆÙ„ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯
            records_to_keep = db_session.query(HistoricalData).filter_by(
                symbol_id=symbol_id,
                date=date
            ).order_by(HistoricalData.id).first()
            
            if records_to_keep:
                db_session.query(HistoricalData).filter_by(
                    symbol_id=symbol_id,
                    date=date
                ).filter(HistoricalData.id != records_to_keep.id).delete()
        
        db_session.commit()
        logger.info(f"âœ… Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯. {len(duplicate_historical)} Ø±Ú©ÙˆØ±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ù Ø´Ø¯.")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ: {e}")
        db_session.rollback()

# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ endpointÙ‡Ø§
# ----------------------------

def run_full_update_with_limits(limit_per_run: int = 100, days_limit: int = 365):
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„ Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ
    """
    try:
        session = get_session_local()
        try:
            run_full_data_update(
                session,
                limit_per_run=limit_per_run,
                days_limit=days_limit
            )
            return True, "Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„: {str(e)}"

def run_historical_update_only(limit_per_run: int = 100, days_limit: int = 365):
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªÙ†Ù‡Ø§ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ
    """
    try:
        session = get_session_local()
        try:
            processed_count, msg = fetch_and_process_historical_data(
                session,
                limit_per_run=limit_per_run,
                days_limit=days_limit
            )
            return True, msg
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ: {str(e)}"

def run_technical_update_only(limit: int = 100):
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªÙ†Ù‡Ø§ Ø¢Ù¾Ø¯ÛŒØª ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
    """
    try:
        session = get_session_local()
        try:
            run_technical_analysis(session, limit=limit)
            return True, "Ø¢Ù¾Ø¯ÛŒØª ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {str(e)}"

def run_fundamental_update_only():
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªÙ†Ù‡Ø§ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ
    """
    try:
        session = get_session_local()
        try:
            update_comprehensive_symbol_data(session)
            return True, "Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {str(e)}"

def get_status_report():
    """
    Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª
    """
    try:
        session = get_session_local()
        try:
            status = get_data_status_report(session)
            consistency = check_data_consistency(session)
            
            report = {
                'status': status,
                'consistency': consistency,
                'timestamp': datetime.now().isoformat(),
                'memory_usage_mb': check_memory_usage_mb()
            }
            
            return True, report
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª: {str(e)}"

def run_data_repair(data_type: str = 'all', limit: int = 50):
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    """
    try:
        session = get_session_local()
        try:
            repair_missing_data(session, data_type=data_type, limit=limit)
            return True, "ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {str(e)}"

def run_cleanup_duplicates():
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
    """
    try:
        session = get_session_local()
        try:
            cleanup_duplicate_data(session)
            return True, "Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ: {str(e)}"

# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù†Ù…Ø§Ø¯Ù‡Ø§
# ----------------------------

def add_single_symbol(db_session: Session, symbol_name: str) -> Tuple[bool, str]:
    """
    Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÛŒÚ© Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¢Ù†.
    """
    logger.info(f"ğŸ“¥ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²ÙˆØ¯Ù† Ù†Ù…Ø§Ø¯ {symbol_name}...")
    
    try:
        # Ú¯Ø§Ù… Û±: Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ù…Ø§Ø¯ Ø§ØµÙ„ÛŒ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±
        import pytse_client as tse
        ticker = tse.Ticker(symbol_name)
        
        tse_index = getattr(ticker, 'index', None)
        if not tse_index:
            return False, f"âš ï¸ Ù†Ù…Ø§Ø¯ {symbol_name} Ø´Ù†Ø§Ø³Ù‡ Ø¨ÙˆØ±Ø³ Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø¶Ø§ÙÙ‡ Ø´ÙˆØ¯."
            
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ù…Ø§Ø¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø± Ø§Ø³Ø§Ø³ tse_index
        existing_symbol = db_session.query(ComprehensiveSymbolData).filter_by(tse_index=tse_index).first()
        if existing_symbol:
            return False, f"â„¹ï¸ Ù†Ù…Ø§Ø¯ {symbol_name} Ø§Ø² Ù‚Ø¨Ù„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯."

        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² tse_index Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† symbol_id
        now = datetime.now()
        new_symbol = ComprehensiveSymbolData(
            symbol_id=tse_index,
            tse_index=tse_index,
            symbol_name=symbol_name,
            company_name=getattr(ticker, 'title', ''),
            isin=getattr(ticker, 'isin', None),
            market_type=getattr(ticker, 'flow', ''),
            group_name=getattr(ticker, 'group_name', ''),
            base_volume=getattr(ticker, 'base_volume', None),
            eps=getattr(ticker, 'eps', None),
            p_e_ratio=getattr(ticker, 'p_e_ratio', None),
            p_s_ratio=getattr(ticker, 'p_s_ratio', None),
            nav=getattr(ticker, 'nav', None),
            float_shares=getattr(ticker, 'float_shares', None),
            market_cap=getattr(ticker, 'market_cap', None),
            industry=getattr(ticker, 'industry', None),
            capital=getattr(ticker, 'capital', None),
            fiscal_year=getattr(ticker, 'fiscal_year', None),
            flow=getattr(ticker, 'flow', None),
            state=getattr(ticker, 'state', None),
            last_historical_update_date=None,
            last_fundamental_update_date=None,
            last_realtime_update=None,
            created_at=now,
            updated_at=now
        )
        
        db_session.add(new_symbol)
        db_session.commit()
        
        logger.info(f"âœ… Ù†Ù…Ø§Ø¯ {symbol_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯. Ø´Ù†Ø§Ø³Ù‡: {new_symbol.symbol_id}")

        # Ú¯Ø§Ù… Û²: Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø§ ØªØ§Ø¨Ø¹ Ø§Ø®ØªØµØ§ØµÛŒ
        hist_updated_count, hist_msg = fetch_and_process_historical_data(db_session, specific_symbols_list=[new_symbol.symbol_name])
        logger.info(f"ğŸ“Š Ø¯ÛŒØªØ§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ {symbol_name}: {hist_updated_count} Ø±Ú©ÙˆØ±Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯. {hist_msg}")
        
        # Ú¯Ø§Ù… Û³: Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø§ ØªØ§Ø¨Ø¹ Ø§Ø®ØªØµØ§ØµÛŒ
        fund_updated_count, fund_msg = update_symbol_fundamental_data(db_session, specific_symbols_list=[new_symbol.symbol_name])
        logger.info(f"ğŸ“Š Ø¯ÛŒØªØ§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {symbol_name}: {fund_updated_count} Ø±Ú©ÙˆØ±Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯. {fund_msg}")
        
        # Ú¯Ø§Ù… Û´: Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø´Ù†Ø§Ø³Ù‡ ØµØ­ÛŒØ­
        calculate_all_indicators(db_session, specific_symbols_list=[new_symbol.symbol_name])
        logger.info(f"ğŸ“ˆ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_name} Ø§Ø¬Ø±Ø§ Ø´Ø¯.")
        
        return True, f"âœ… Ù†Ù…Ø§Ø¯ {symbol_name} Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¢Ù† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù†Ø¯."

    except Exception as e:
        db_session.rollback()
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÙØ²ÙˆØ¯Ù† Ù†Ù…Ø§Ø¯ {symbol_name}: {e}")
        return False, f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÙØ²ÙˆØ¯Ù† Ù†Ù…Ø§Ø¯: {str(e)}"


def remove_symbol(symbol_name: str):
    """
    Ø­Ø°Ù ÛŒÚ© Ù†Ù…Ø§Ø¯ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    """
    try:
        session = get_session_local()
        try:
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù†Ù…Ø§Ø¯ Ø¯Ø± ComprehensiveSymbolData
            symbol = session.query(ComprehensiveSymbolData).filter_by(symbol_id=symbol_name).first()
            if not symbol:
                return False, "Ù†Ù…Ø§Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯"
            
            # Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ø¨Ø³ØªÙ‡
            session.query(HistoricalData).filter_by(symbol_id=symbol.id).delete()
            session.query(TechnicalIndicatorData).filter_by(symbol_id=symbol.id).delete()
            session.query(FundamentalData).filter_by(symbol_id=symbol.id).delete()
            
            # Ø­Ø°Ù Ø®ÙˆØ¯ Ù†Ù…Ø§Ø¯
            session.delete(symbol)
            session.commit()
            
            return True, f"Ù†Ù…Ø§Ø¯ {symbol_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø­Ø°Ù Ø´Ø¯"
            
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù Ù†Ù…Ø§Ø¯ {symbol_name}: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù Ù†Ù…Ø§Ø¯: {str(e)}"

def update_symbol_info(symbol_name: str):
    """
    Ø¢Ù¾Ø¯ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒÚ© Ù†Ù…Ø§Ø¯
    """
    try:
        session = get_session_local()
        try:
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù†Ù…Ø§Ø¯ Ø¯Ø± ComprehensiveSymbolData
            symbol = session.query(ComprehensiveSymbolData).filter_by(symbol_id=symbol_name).first()
            if not symbol:
                return False, "Ù†Ù…Ø§Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯"
            
            # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù‡ Ø±ÙˆØ² Ø§Ø² pytse-client
            ticker = tse.Ticker(symbol_name)
            if not ticker:
                return False, "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ù…Ø§Ø¯"
            
            # Ø¢Ù¾Ø¯ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ù…Ø§Ø¯
            symbol.symbol_name = getattr(ticker, 'title', symbol_name)
            symbol.company_name = getattr(ticker, 'company_name', symbol.company_name)
            symbol.market_type = getattr(ticker, 'market', symbol.market_type)
            symbol.group_name = getattr(ticker, 'group_name', symbol.group_name)
            symbol.base_volume = getattr(ticker, 'base_volume', symbol.base_volume)
            symbol.updated_at = datetime.now()
            
            session.commit()
            
            return True, f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ù…Ø§Ø¯ {symbol_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù¾Ø¯ÛŒØª Ø´Ø¯"
            
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ù…Ø§Ø¯ {symbol_name}: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ù…Ø§Ø¯: {str(e)}"

# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ ÙÛŒÙ„ØªØ± Ùˆ Ø¬Ø³ØªØ¬Ùˆ
# ----------------------------

def search_symbols(query: str, limit: int = 20):
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… ÛŒØ§ Ù†Ù…Ø§Ø¯
    """
    try:
        session = get_session_local()
        try:
            results = session.query(ComprehensiveSymbolData).filter(
                (ComprehensiveSymbolData.symbol_name.ilike(f"%{query}%")) |
                (ComprehensiveSymbolData.symbol_id.ilike(f"%{query}%")) |
                (ComprehensiveSymbolData.company_name.ilike(f"%{query}%"))
            ).limit(limit).all()
            
            symbols_list = []
            for symbol in results:
                symbols_list.append({
                    'id': symbol.id,
                    'symbol_id': symbol.symbol_id,
                    'symbol_name': symbol.symbol_name,
                    'company_name': symbol.company_name,
                    'market_type': symbol.market_type,
                    'created_at': symbol.created_at.isoformat() if symbol.created_at else None
                })
            
            return True, symbols_list
            
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§: {str(e)}"


def filter_symbols_by_market(market: str, limit: int = 100):
    """
    ÙÛŒÙ„ØªØ± Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø§Ø²Ø§Ø±
    """
    try:
        session = get_session_local()
        try:
            results = session.query(ComprehensiveSymbolData).filter(
                ComprehensiveSymbolData.market_type.ilike(f"%{market}%")
            ).limit(limit).all()
            
            symbols_list = []
            for symbol in results:
                symbols_list.append({
                    'id': symbol.id,
                    'symbol_id': symbol.symbol_id,
                    'symbol_name': symbol.symbol_name,
                    'market_type': symbol.market_type,
                    'company_name': symbol.company_name
                })
            
            return True, symbols_list
            
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙÛŒÙ„ØªØ± Ù†Ù…Ø§Ø¯Ù‡Ø§: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± ÙÛŒÙ„ØªØ± Ù†Ù…Ø§Ø¯Ù‡Ø§: {str(e)}"

    
def get_symbol_comprehensive_report(symbol_identifier: str):
    """
    Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯ (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² symbol_id ÛŒØ§ symbol_name)
    """
    try:
        session = get_session_local()
        try:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ù…Ø§Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ symbol_id ÛŒØ§ symbol_name
            symbol = session.query(ComprehensiveSymbolData).filter(
                (ComprehensiveSymbolData.symbol_id == symbol_identifier) |
                (ComprehensiveSymbolData.symbol_name == symbol_identifier)
            ).first()
            
            if not symbol:
                return False, "Ù†Ù…Ø§Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯"
            
            # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡
            symbol_info = {
                'id': symbol.id,
                'symbol_id': symbol.symbol_id,
                'symbol_name': symbol.symbol_name,
                'company_name': symbol.company_name,
                'market_type': symbol.market_type,
                'industry': symbol.industry,
                'group_name': symbol.group_name,
                'base_volume': symbol.base_volume,
                'created_at': symbol.created_at.isoformat() if symbol.created_at else None
            }
            
            # Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ§Ø±ÛŒØ®ÛŒ
            historical_data = session.query(HistoricalData).filter_by(
                symbol_id=symbol.id
            ).order_by(HistoricalData.date.desc()).limit(30).all()
            
            historical_list = []
            for hist in historical_data:
                historical_list.append({
                    'date': hist.date.isoformat() if hist.date else None,
                    'jdate': hist.jdate,
                    'open': hist.open,
                    'high': hist.high,
                    'low': hist.low,
                    'close': hist.close,
                    'volume': hist.volume,
                    'value': hist.value
                })
            
            # Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÚ©Ù†ÛŒÚ©Ø§Ù„
            technical_data = session.query(TechnicalIndicatorData).filter_by(
                symbol_id=symbol.id
            ).order_by(TechnicalIndicatorData.jdate.desc()).first()
            
            technical_info = {}
            if technical_data:
                technical_info = {
                    'RSI': technical_data.RSI,
                    'MACD': technical_data.MACD,
                    'MACD_Signal': technical_data.MACD_Signal,
                    'MACD_Hist': technical_data.MACD_Hist,
                    'SMA_20': technical_data.SMA_20,
                    'SMA_50': technical_data.SMA_50,
                    'Bollinger_High': technical_data.Bollinger_High,
                    'Bollinger_Low': technical_data.Bollinger_Low,
                    'Bollinger_MA': technical_data.Bollinger_MA,
                    'Volume_MA_20': technical_data.Volume_MA_20,
                    'ATR': technical_data.ATR,
                    'jdate': technical_data.jdate
                }
            
            # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù†ÛŒØ§Ø¯ÛŒ
            fundamental_data = session.query(FundamentalData).filter_by(
                symbol_id=symbol.id
            ).first()
            
            fundamental_info = {}
            if fundamental_data:
                fundamental_info = {
                    'eps': fundamental_data.eps,
                    'pe': fundamental_data.pe,
                    'group_pe_ratio': fundamental_data.group_pe_ratio,
                    'psr': fundamental_data.psr,
                    'p_s_ratio': fundamental_data.p_s_ratio,
                    'market_cap': fundamental_data.market_cap,
                    'base_volume': fundamental_data.base_volume,
                    'float_shares': fundamental_data.float_shares
                }
            
            report = {
                'symbol_info': symbol_info,
                'historical_data': historical_list,
                'technical_indicators': technical_info,
                'fundamental_data': fundamental_info,
                'report_date': datetime.now().isoformat()
            }
            
            return True, report
            
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø¨Ø±Ø§ÛŒ {symbol_identifier}: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹: {str(e)}"

def get_market_summary():
    """
    Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ø¨Ø§Ø²Ø§Ø±
    """
    try:
        session = get_session_local()
        try:
            # ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø¨Ø§Ø²Ø§Ø±
            market_stats = session.query(
                ComprehensiveSymbolData.market_type,
                func.count(ComprehensiveSymbolData.id)
            ).group_by(ComprehensiveSymbolData.market_type).all()
            
            # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† P/E Ùˆ P/B
            avg_pe = session.query(
                func.avg(FundamentalData.pe)
            ).filter(FundamentalData.pe.isnot(None)).scalar()
            
            avg_pb = session.query(
                func.avg(FundamentalData.p_b)
            ).filter(FundamentalData.p_b.isnot(None)).scalar()
            
            # ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ Ú©Ø§Ù…Ù„
            symbols_with_complete_data = session.query(ComprehensiveSymbolData).filter(
                ComprehensiveSymbolData.id.in_(session.query(HistoricalData.symbol_id).distinct()),
                ComprehensiveSymbolData.id.in_(session.query(TechnicalIndicatorData.symbol_id).distinct()),
                ComprehensiveSymbolData.id.in_(session.query(FundamentalData.symbol_id).distinct())
            ).count()
            
            summary = {
                'total_symbols': session.query(ComprehensiveSymbolData).count(),
                'symbols_with_historical': session.query(func.count(distinct(HistoricalData.symbol_id))).scalar(),
                'symbols_with_technical': session.query(func.count(distinct(TechnicalIndicatorData.symbol_id))).scalar(),
                'symbols_with_fundamental': session.query(func.count(distinct(FundamentalData.symbol_id))).scalar(),
                'symbols_with_complete_data': symbols_with_complete_data,
                'market_distribution': {market: count for market, count in market_stats},
                'average_pe': float(avg_pe) if avg_pe else None,
                'average_pb': float(avg_pb) if avg_pb else None,
                'last_updated': datetime.now().isoformat()
            }
            
            return True, summary
            
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§Ø²Ø§Ø±: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§Ø²Ø§Ø±: {str(e)}"

# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±
# ----------------------------

def schedule_daily_update():
    """
    Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡
    """
    try:
        logger.info("â° Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡...")
        
        # Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§
        success, msg = run_historical_update_only(limit_per_run=200, days_limit=1)
        if not success:
            logger.warning(f"âš ï¸ Ø¢Ù¾Ø¯ÛŒØª ØªØ§Ø±ÛŒØ®ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {msg}")
        
        # Ø¢Ù¾Ø¯ÛŒØª ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
        success, msg = run_technical_update_only(limit=200)
        if not success:
            logger.warning(f"âš ï¸ Ø¢Ù¾Ø¯ÛŒØª ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {msg}")
        
        logger.info("âœ… Ø¢Ù¾Ø¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        return True, "Ø¢Ù¾Ø¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø±ÙˆØ²Ø§Ù†Ù‡: {str(e)}"

def schedule_weekly_update():
    """
    Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ù‡ÙØªÚ¯ÛŒ
    """
    try:
        logger.info("â° Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª Ù‡ÙØªÚ¯ÛŒ...")
        
        # Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        success, msg = run_full_update_with_limits(limit_per_run=300, days_limit=7)
        if not success:
            logger.warning(f"âš ï¸ Ø¢Ù¾Ø¯ÛŒØª Ù‡ÙØªÚ¯ÛŒ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {msg}")
        
        # ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ø±ÙØªÙ‡
        success, msg = run_data_repair(data_type='all', limit=100)
        if not success:
            logger.warning(f"âš ï¸ ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‡ÙØªÚ¯ÛŒ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {msg}")
        
        logger.info("âœ… Ø¢Ù¾Ø¯ÛŒØª Ù‡ÙØªÚ¯ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        return True, "Ø¢Ù¾Ø¯ÛŒØª Ù‡ÙØªÚ¯ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ù‡ÙØªÚ¯ÛŒ: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ù‡ÙØªÚ¯ÛŒ: {str(e)}"

def schedule_monthly_maintenance():
    """
    Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…Ø§Ù‡Ø§Ù†Ù‡
    """
    try:
        logger.info("â° Ø´Ø±ÙˆØ¹ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…Ø§Ù‡Ø§Ù†Ù‡...")
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        success, msg = run_cleanup_duplicates()
        if not success:
            logger.warning(f"âš ï¸ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…Ø§Ù‡Ø§Ù†Ù‡ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {msg}")
        
        # Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ
        success, msg = run_fundamental_update_only()
        if not success:
            logger.warning(f"âš ï¸ Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù…Ø§Ù‡Ø§Ù†Ù‡ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {msg}")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª
        success, report = get_status_report()
        if success:
            logger.info(f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ù…Ø§Ù‡Ø§Ù†Ù‡: {report}")
        
        logger.info("âœ… Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…Ø§Ù‡Ø§Ù†Ù‡ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        return True, "Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…Ø§Ù‡Ø§Ù†Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯"
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…Ø§Ù‡Ø§Ù†Ù‡: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…Ø§Ù‡Ø§Ù†Ù‡: {str(e)}"

# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ùˆ Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ
# ----------------------------

def debug_symbol_data(symbol_name: str):
    """
    Ø¯ÛŒØ¨Ø§Ú¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒÚ© Ù†Ù…Ø§Ø¯
    """
    try:
        session = get_session_local()
        try:
            symbol = session.query(ComprehensiveSymbolData).filter_by(name=symbol_name).first()
            if not symbol:
                return False, "Ù†Ù…Ø§Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯"
            
            debug_info = {
                'symbol': {
                    'id': symbol.id,
                    'name': symbol.name,
                    'tse_index': symbol.tse_index,
                    'market': symbol.market
                },
                'historical_count': session.query(HistoricalData).filter_by(symbol_id=symbol.id).count(),
                'technical_count': session.query(TechnicalIndicatorData).filter_by(symbol_id=symbol.id).count(),
                'fundamental_exists': session.query(FundamentalData).filter_by(symbol_id=symbol.id).first() is not None,
                'latest_historical': session.query(HistoricalData).filter_by(symbol_id=symbol.id).order_by(HistoricalData.date.desc()).first(),
                'latest_technical': session.query(TechnicalIndicatorData).filter_by(symbol_id=symbol.id).order_by(TechnicalIndicatorData.date.desc()).first()
            }
            
            return True, debug_info
            
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯ÛŒØ¨Ø§Ú¯ Ù†Ù…Ø§Ø¯ {symbol_name}: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯ÛŒØ¨Ø§Ú¯ Ù†Ù…Ø§Ø¯: {str(e)}"

def test_pytse_connection():
    """
    ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ pytse-client
    """
    try:
        if not pytse_wrapper.is_available():
            return False, "pytse-client Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
        
        # ØªØ³Øª Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…Ø§Ø¯Ù‡Ø§
        symbols = pytse_wrapper.get_all_symbols()
        if not symbols:
            return False, "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø§Ø² pytse-client"
        
        # ØªØ³Øª Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯
        test_symbol = list(symbols.keys())[0] if symbols else None
        if test_symbol:
            ticker = pytse_wrapper.get_ticker(test_symbol)
            if not ticker:
                return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ù…Ø§Ø¯ {test_symbol}"
        
        return True, "Ø§ØªØµØ§Ù„ Ø¨Ù‡ pytse-client Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªØ³Øª Ø´Ø¯"
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ pytse-client: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª Ø§ØªØµØ§Ù„: {str(e)}"

def test_database_connection():
    """
    ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    """
    try:
        session = get_session_local()
        try:
            # ØªØ³Øª query Ø³Ø§Ø¯Ù‡
            count = session.query(ComprehensiveSymbolData).count()
            return True, f"Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ². ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§: {count}"
        finally:
            session.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
        return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(e)}"

# ----------------------------
# ØªÙˆØ§Ø¨Ø¹ main Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…
# ----------------------------

def main():
    """
    ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø¨ÙˆØ±Ø³')
    parser.add_argument('--full-update', action='store_true', help='Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„')
    parser.add_argument('--historical', action='store_true', help='Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª ØªØ§Ø±ÛŒØ®ÛŒ')
    parser.add_argument('--technical', action='store_true', help='Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª ØªÚ©Ù†ÛŒÚ©Ø§Ù„')
    parser.add_argument('--fundamental', action='store_true', help='Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ')
    parser.add_argument('--repair', action='store_true', help='Ø§Ø¬Ø±Ø§ÛŒ ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§')
    parser.add_argument('--cleanup', action='store_true', help='Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ')
    parser.add_argument('--status', action='store_true', help='Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª')
    parser.add_argument('--limit', type=int, default=100, help='Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§')
    parser.add_argument('--days', type=int, default=365, help='Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø±ÙˆØ²Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ')
    
    args = parser.parse_args()
    
    try:
        if args.full_update:
            success, msg = run_full_update_with_limits(args.limit, args.days)
            print(f"Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„: {'âœ… Ù…ÙˆÙÙ‚' if success else 'âŒ Ø®Ø·Ø§'} - {msg}")
        
        elif args.historical:
            success, msg = run_historical_update_only(args.limit, args.days)
            print(f"Ø¢Ù¾Ø¯ÛŒØª ØªØ§Ø±ÛŒØ®ÛŒ: {'âœ… Ù…ÙˆÙÙ‚' if success else 'âŒ Ø®Ø·Ø§'} - {msg}")
        
        elif args.technical:
            success, msg = run_technical_update_only(args.limit)
            print(f"Ø¢Ù¾Ø¯ÛŒØª ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {'âœ… Ù…ÙˆÙÙ‚' if success else 'âŒ Ø®Ø·Ø§'} - {msg}")
        
        elif args.fundamental:
            success, msg = run_fundamental_update_only()
            print(f"Ø¢Ù¾Ø¯ÛŒØª Ø¨Ù†ÛŒØ§Ø¯ÛŒ: {'âœ… Ù…ÙˆÙÙ‚' if success else 'âŒ Ø®Ø·Ø§'} - {msg}")
        
        elif args.repair:
            success, msg = run_data_repair('all', args.limit)
            print(f"ØªØ¹Ù…ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {'âœ… Ù…ÙˆÙÙ‚' if success else 'âŒ Ø®Ø·Ø§'} - {msg}")
        
        elif args.cleanup:
            success, msg = run_cleanup_duplicates()
            print(f"Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ: {'âœ… Ù…ÙˆÙÙ‚' if success else 'âŒ Ø®Ø·Ø§'} - {msg}")
        
        elif args.status:
            success, result = get_status_report()
            if success:
                import json
                print(json.dumps(result, indent=2, ensure_ascii=False))
            else:
                print(f"âŒ Ø®Ø·Ø§: {result}")
        
        else:
            parser.print_help()
            
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø³ØªÙˆØ±: {e}")
        print(f"Ø®Ø·Ø§: {e}")

if __name__ == "__main__":
    main()

# ----------------------------
# Export functions for Flask app
# ----------------------------

__all__ = [
    # ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
    'setup_robust_session'

     # ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ   
    'populate_comprehensive_symbols'
    'fetch_historical_and_fundamental_data'
    'fetch_symbols_from_pytse_client',
    'fetch_and_process_historical_data',
    #'update_historical_data_limited',(DELETED)
    'run_full_data_update',
    'run_technical_analysis',
    'update_comprehensive_symbol_data',
    'initial_populate_all_symbols_and_data',
    
    # ØªÙˆØ§Ø¨Ø¹ Ù…Ø¯ÛŒØ±ÛŒØª
    'run_full_update_with_limits',
    'run_historical_update_only',
    'run_technical_update_only',
    'run_fundamental_update_only',
    'get_status_report',
    'run_data_repair',
    'run_cleanup_duplicates',
    
    # ØªÙˆØ§Ø¨Ø¹ Ù†Ù…Ø§Ø¯Ù‡Ø§
    'add_single_symbol',
    'remove_symbol',
    'update_symbol_info',
    'search_symbols',
    'filter_symbols_by_market',
    
    # ØªÙˆØ§Ø¨Ø¹ Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ
    'get_symbol_comprehensive_report',
    'get_market_summary',
    
    # ØªÙˆØ§Ø¨Ø¹ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ
    'schedule_daily_update',
    'schedule_weekly_update',
    'schedule_monthly_maintenance',
    
    # ØªÙˆØ§Ø¨Ø¹ Ø¯ÛŒØ¨Ø§Ú¯
    'debug_symbol_data',
    'test_pytse_connection',
    'test_database_connection',
    
    # utility
    'get_session_local',
    'cleanup_memory',
    'get_symbol_id'
]                        
